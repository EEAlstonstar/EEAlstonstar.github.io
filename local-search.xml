<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>深度学习基础（下）</title>
    <link href="/2023/07/14/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%EF%BC%88%E4%B8%8B%EF%BC%89/"/>
    <url>/2023/07/14/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%EF%BC%88%E4%B8%8B%EF%BC%89/</url>
    
    <content type="html"><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>今天继续来弄深度学习。主要就是详细探讨一下常用网络什么的。应该会另开一篇讲讲Attention和Tranformer机制，这块比较重要。这样也就差不多能入门了。</p><p>至于周末，可以开始学一下对比学习什么的。这样算下来。基本到15号基础知识就复习的差不多了。在学一下BERT这些，最多20号吧。然后7月就是跑BYOL-A。希望效果好一点吧。惋惜。</p><p>哎哟要我说本科生能做的工作无非是证明虽然铲子能挖土，但铁片+锤锤也能挖土（Tranformer+CNN&#x3D;VIT），真是的，要不是为了有学上谁来应付这个。</p><p>真是新时代摘棉花、21世纪流水线女工、赛博朋克富士康。</p><hr><h1 id="一、常见优化方法总结"><a href="#一、常见优化方法总结" class="headerlink" title="一、常见优化方法总结"></a>一、常见优化方法总结</h1>]]></content>
    
    
    
    <tags>
      
      <tag>CV</tag>
      
      <tag>神经网络</tag>
      
      <tag>深度学习</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>深度学习基础(上)</title>
    <link href="/2023/07/13/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/"/>
    <url>/2023/07/13/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/</url>
    
    <content type="html"><![CDATA[<h1 id="前言部分"><a href="#前言部分" class="headerlink" title="前言部分"></a>前言部分</h1><p>哎哟自从开始实习之后，就觉得深度学习真的越来越不香了，申NUS又需要科研bg，没办法重操就业。打算暑假发完就再也不碰了。有一种被迫跟前妻生孩子的感觉，，，好荒谬。</p><p>具体安排就是暑假先弄BYOL啥的，学一下Hadoop和GaussDB吧，科研bg暑假结束算是走完了。然后下学期开始考GRE啥的吧，提提GPA。然后寒假开始还是弄弄实习经历吧。<br>感觉下周开始就得做实验了。</p><p>啊！我88均分的烂GPA可怎么有学上啊！</p><p>那就开整吧。</p><p>本篇文章将大体简略概述深度学习基础。主要目的是复习。因此深度不深，广度比较高。因此本篇适合有深度学习基础的经验者。</p><hr><h1 id="一、什么是深度学习？"><a href="#一、什么是深度学习？" class="headerlink" title="一、什么是深度学习？"></a>一、什么是深度学习？</h1><ul><li><p><strong>定义</strong>：一般是指训练多层网络结构对未知数据进行分类或者回归</p></li><li><p><strong>分类</strong>：</p><ol><li><strong>有监督学习方法</strong>：有标签的那种，前馈网络、卷积神经网络、循环神经网络。</li><li><strong>无监督学习方法</strong>：自编码器啥的。</li></ol></li><li><p><strong>思想</strong>：<br>  通过构建多层网络，对目标进行多层表示，以期通过多层的高层次特征来表示数据的抽象语义信息，获得更好特征鲁棒性。</p><blockquote><p><strong>鲁棒性：</strong> 鲁棒是Robust的音译，也就是健壮和强壮的意思。它也是在异常和危险情况下系统生存的能力。<br>            AI模型的鲁棒可以理解为模型对数据变化的容忍度。假设数据出现较小偏差，只对模型输出产生较小的影响，则称模型是鲁棒的。  </p></blockquote></li><li><p><strong>主要应用</strong>：都在应用。图像处理（cv）：分类检测回归识别。语音识别、nlp什么的。</p></li><li><p><strong>主要术语</strong>: </p><ol><li><strong>泛化能力</strong>：是指模型依据训练时采用的模型，针对未见过的新数据做出争取预测的能力。</li><li><strong>过拟合</strong>：创建的模型与训练数据过于匹配，以至于模型无法根据新数据做出正确的预测。</li><li><strong>batch</strong>：模型训练一次迭代使用的样本集</li><li><strong>参数</strong>：机器学习自己训练得到的模型变量，例如：权重；</li><li><strong>超参数</strong>：在模型训练的连续过程中，需要人工指正的。eg：学习率</li><li><strong>特征工程</strong>：指确定哪些特征可能在训练模型方面非常有用，然后将日志文件及其他来源的原始数据转换为所需的特征。特征工程有时称为特征提取。</li><li><strong>l1正则化</strong>：一种正则化，根据权重的绝对值的总和，来惩罚权重。在以来稀疏特征的模型中，L1正则化有助于使不相关或几乎不相关的特征的权重正好为0，从而将这些特征从模型中移除。与L2正则化相对。</li><li><strong>l2正则化</strong>：一种正则化，根据权重的平方和，来惩罚权重。L2正则化有助于使离群值（具有较大正值或较小负责）权重接近于0，但又不正好为0。在线性模型中，L2正则化始终可以进行泛化。</li></ol></li></ul><hr><h1 id="二、神经网络基础"><a href="#二、神经网络基础" class="headerlink" title="二、神经网络基础"></a>二、神经网络基础</h1><h2 id="2-1-神经网络组成"><a href="#2-1-神经网络组成" class="headerlink" title="2.1 神经网络组成"></a>2.1 神经网络组成</h2><p><strong>感知机</strong>——&gt;<strong>神经元</strong><br><img src="/2023/07/13/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/img.png" alt="感知机模型.png"><br>这个是一个很简单的感知机模型。<br>可以理解成：我们人的神经元首先接收到一些信号，这些信号通过这些树突(dendrite)组织，<br>树突组织接收到这些信号送到细胞里边的细胞核(nucleus)，这些细胞核对接收到的这些信号， 比如说眼睛接收到的光学啊，或者耳朵接收到的声音信号，<br>到树突的时候会产生一些微弱的生物电，那么就形成这样的一些刺激，<br>那么在细胞核里边对这些收集到的接收到的刺激进行综合的处理，当他的信号达到了一定的阈值之后，那么他就会被激活，就会产生一个刺激的输出。</p><p>一个简单的感知机单元的与非门表示如下：<br><img src="/2023/07/13/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/img_1.png" alt="感知机与非门.png"><br>当输入为0，1时，感知机输出为0<em>（-2）+1</em>（-2）+3&#x3D;1</p><p>复杂一些的感知机由简单的感知机单元组合而成：<br><img src="/2023/07/13/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/img_2.png" alt="复杂点的感知机模型.png"></p><hr><p><strong>多层感知机</strong>  </p><p>多层感知机由感知机推广而来，最主要的特点是有多个神经元层，因此也叫深度神经网络。相比于单独的感知机，多层感知机的第i层的每个神经元和第i-1层的每个神经元都有连接。<br><img src="/2023/07/13/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/img_3.png" alt="多层感知机.png"></p><p>当然输出层可以不止有1个神经元：</p><p><img src="/2023/07/13/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/img_4.png" alt="多输出层.png"></p><hr><h2 id="2-2-关于前向传播"><a href="#2-2-关于前向传播" class="headerlink" title="2.2 关于前向传播"></a>2.2 关于前向传播</h2><p>神经网络的计算主要有两种：  </p><ol><li><strong>前向传播（FP）</strong>：作用于每一层的输入,通过逐层计算得到输出结果</li><li><strong>反向传播（BP）</strong>：作用于网络的输出，通过计算梯度由深到浅更新网络参数</li></ol><p><strong>前向传播</strong>：<br><img src="/2023/07/13/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/img_5.png" alt="前向传播.png"></p><p><strong>反向传播</strong>：<br>这个有点复杂，具体来说流程就是，正向传播求损失，反向传播回传误差，然后求梯度。更新参数。<br>简单举个例子吧：<br><img src="/2023/07/13/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/img_6.png" alt="反向传播.png"><br>以这张图为例：<br>上图的前向传播（网络输出计算）过程如下：<br><img src="/2023/07/13/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/img_7.png" alt="计算流程.png"></p><p>误差的计算方法是mse：<br><img src="/2023/07/13/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/img_8.png" alt="mse.png"></p><p>具体来说们就是步步拆开的过程，先是总体求E，只需要注意每一个神经元中还有激活函数，netj和yj不是一个东西。</p><p>具体的计算过程是<strong>梯度下降</strong>算法：<br><img src="/2023/07/13/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/img_9.png" alt="梯度下降算法.png"><br>举一个具体例子进行阐述吧：<br><img src="/2023/07/13/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/img_10.png"><br>前向运算的过程如下：（使用的是sigmoid激活函数）<br><img src="/2023/07/13/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/img_11.png"><br>下面是反向传播过程，首先要明确这是一个<strong>链式求导</strong>的过程：<br><img src="/2023/07/13/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/img_12.png" alt="链式求导.png"><br>然后是参数更新过程：<br><img src="/2023/07/13/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/img_13.png" alt="参数更新.png"></p><hr><h1 id="三、超参数"><a href="#三、超参数" class="headerlink" title="三、超参数"></a>三、超参数</h1><h2 id="3-1-什么是超参数？"><a href="#3-1-什么是超参数？" class="headerlink" title="3.1 什么是超参数？"></a>3.1 什么是超参数？</h2><ul><li><strong>超参数</strong>：在机器学习的上下文中，超参数是在开始学习过程之前设置值的参数，而不是通过训练得到的参数数据。<br>通常情况下，需要对超参数进行优化，给学习机选择一组最优超参数，以提高学习的性能和效果。</li></ul><p>这里的重点主要在于：<strong>什么是学习率？</strong><br>以上文所提到的梯度下降来说，求梯度前的n就是学习率a。<br>可以很清晰地看到，学习率决定了梯度下降的快慢，以吴恩达机器学习课程为例：<br><img src="/2023/07/13/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/img_14.png" alt="吴恩达机器学习.png"><br>我们先看一下上面出现的唯一一条公式，公式中的θ就是代表着权重参数，新的θ会由之前的θ计算得来，这个计算过程就是为了寻找目标函数收敛到最小值。那么公式中出现的α就是当下的学习率。</p><p>上面两张明显的对比图就给出了学习率过大和过小的情况。</p><ul><li><p>学习率设置<strong>过小</strong>的时候，每步太小，下降速度太慢，可能要花很长的时间才会找到最小值。</p></li><li><p>学习率设置<strong>过大</strong>的时候，每步太大，虽然收敛得速度很快，可能会像图中一样，跨过或忽略了最小值，导致一直来回震荡而无法收敛。</p></li></ul><hr><h1 id="四、激活函数"><a href="#四、激活函数" class="headerlink" title="四、激活函数"></a>四、激活函数</h1><h2 id="4-1-什么是激活函数？"><a href="#4-1-什么是激活函数？" class="headerlink" title="4.1 什么是激活函数？"></a>4.1 什么是激活函数？</h2><p><strong>这个可以好好说一下</strong><br>激活函数(Activation functions)对于人工神经网络模型去学习、理解非常复杂和非线性的函数来说具有十分重要的作用。</p><p>它们将<strong>非线性特性</strong>引入到我们的网络中。</p><p>如下图，在神经元中，输入的 inputs 通过加权，求和后，还被作用了一个函数，这个函数就是激活函数。</p><p>引入激活函数是为了增加神经网络模型的非线性。没有激活函数的每层都相当于矩阵相乘。就算你叠加了若干层之后，无非还是个矩阵相乘罢了。</p><p><img src="/2023/07/13/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/img_15.png" alt="为什么需要有激活函数.png"></p><hr><h2 id="4-2为什么要使用激活函数？"><a href="#4-2为什么要使用激活函数？" class="headerlink" title="4.2为什么要使用激活函数？"></a>4.2为什么要使用激活函数？</h2><ol><li>激活函数对模型学习、理解非常复杂和非线性的函数具有重要作用。</li><li><strong>激活函数可以引入非线性因素</strong>。如果不使用激活函数，则输出信号仅是一个简单的线性函数。<blockquote><p>线性函数一个一级多项式，线性方程的复杂度有限，从数据中学习复杂函数映射的能力很小。没有激活函数，神经网络将无法学习和模拟其他复杂类型的数据，例如图像、视频、音频、语音等。</p></blockquote></li><li>激活函数可以把当前特征空间通过一定的线性映射转换到另一个空间，让数据能够更好的被分类。</li></ol><h2 id="4-3-为什么激活函数需要非线性函数？"><a href="#4-3-为什么激活函数需要非线性函数？" class="headerlink" title="4.3 为什么激活函数需要非线性函数？"></a>4.3 为什么激活函数需要非线性函数？</h2><ul><li>假若网络中全部是线性部件，那么线性的组合还是线性，与单独一个线性分类器无异。这样就做不到用非线性来逼近任意函数。    </li><li>使用非线性激活函数，以便使网络更加强大，增加它的能力，使它可以学习<strong>复杂的事物</strong>，复杂的表单数据，以及表示输入输出之间非线性的复杂的任意函数映射。  </li><li>使用非线性激活函数，能够从输入输出之间生成<strong>非线性映射</strong>。</li></ul><h2 id="4-4-常见激活函数"><a href="#4-4-常见激活函数" class="headerlink" title="4.4 常见激活函数"></a>4.4 常见激活函数</h2><p><img src="/2023/07/13/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/img_16.png" alt="常见激活函数.png"></p><p><img src="/2023/07/13/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/img_17.png" alt="常见激活函数.png"></p><p>等等。挺多的。  </p><p>关于激活函数的选择，有一个经验法则，很玄学：</p><blockquote><p>如果输出是0、1值（二分类问题），则输出层选择 sigmoid 函数，然后其它的所有单元都选择 Relu 函数。这是很多激活函数的默认选择，如果在隐藏层上不确定使用哪个激活函数，那么通常会使用 Relu 激活函数。有时，也会使用 tanh 激活函数。</p></blockquote><blockquote><p>对于多分类问题，一般输出层选择softmax激活函数</p></blockquote><blockquote><p>Tanh：它非常优秀，几乎适合所有场合</p></blockquote><blockquote><p>ReLU：最常用的激活函数，如果不确定用哪个激活函数，就先使用ReLU或者Leaky ReLU，再尝试其他激活函数</p></blockquote><hr><h2 id="4-5-一些常见问题"><a href="#4-5-一些常见问题" class="headerlink" title="4.5 一些常见问题"></a>4.5 一些常见问题</h2><ul><li><em><strong>为什么总是能看到Relu激活函数？</strong></em></li></ul><p>答：relu函数的优点如下：</p><ul><li><strong>训练更快</strong>：使用ReLU或者Leaky ReLU的神经网络通常会比使用sigmoid或tanh的神经网络学习得更快</li><li><strong>避免了梯度弥散</strong>：当输入无穷大或无穷小时，sigmoid或tanh的梯度接近0，这会造成梯度弥散现象。<strong>而ReLU或者Leaky ReLU在输入大于0的部分，导数均为一个常数，避免了梯度弥散。</strong></li><li><strong>单侧抑制，稀疏性</strong>：当输入小于0时，ReLU的梯度为0，神经元不会训练。模拟神经元的激活率是很低的这一特性，对输入信号的少部分进行选择性相应，正是这样的稀疏性提高了网络的性能。</li></ul><p>relu函数的缺点如下：</p><ul><li>在输入小于0的时候，即使有很大的梯度传播过来也会戛然而止。</li></ul><hr><ul><li><em><strong>为什么为什么tanh收敛速度比sigmoid快？</strong></em></li></ul><p>这与激活函数的求导有关：</p><p>tanh′(x)&#x3D;1−tanh(x)^2 ，导数范围在（0，1）</p><p>sigmoid’(x)&#x3D;s(x)(1-s(x))sigmoid<br>′<br> (x)&#x3D;s(x)(1−s(x))，导数范围在（0，1&#x2F;4）</p><p>由此可见，tanh(x)梯度消失的问题比sigmoid(x)轻，所以tanh收敛速度比较快。</p><hr><h1 id="五、损失函数"><a href="#五、损失函数" class="headerlink" title="五、损失函数"></a>五、损失函数</h1><h2 id="5-1-损失函数定义"><a href="#5-1-损失函数定义" class="headerlink" title="5.1 损失函数定义"></a>5.1 损失函数定义</h2><p>在机器学习任务中，大部分监督学习算法都会有一个目标函数 (Objective Function),算法对该目标函数进行优化，称为优化算法的过程。 例如在分类或者回归任务中，使用损失函数( Loss Function )作为其目标函数对算法模型进行优化 。  </p><p>在BP神经网络中，一般推导中，使用均方误差作为损失函数，而在实际中，常用交叉熵作为损失函数。如下图所示，我们可以清晰地观察到不同的损失函数在梯度下降过程中的收敛速度和性能都是不同的。</p><ol><li>均方误差作为损失函数收敛速度慢，可能会陷入局部最优解；</li><li>而交叉熵作为损失函数的收敛速度比均方误差快，且较为容易找到函数最优解.</li></ol><p>因此了解损失函数的类型并掌握损失函数的使用技巧，有助于加深对深度学习的认知.</p><p><img src="/2023/07/13/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/img0.png"></p><p> 对于分类和回归模型使用的损失函数不同，下面将分别进行介绍。</p><hr><h2 id="5-2-回归损失函数"><a href="#5-2-回归损失函数" class="headerlink" title="5.2 回归损失函数"></a>5.2 回归损失函数</h2><p>(1)<strong>均方误差损失函数</strong></p><p>均方误差(Mean Squared Error Loss, MSE)损失函数定义如下：<br><img src="/2023/07/13/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/img1.png" alt="mse.png"></p><p>这里说一下MSE是无法处理梯度消失问题的。先说说什么是梯度消失。</p><blockquote><p>梯度消失是在神经网络训练过程中经常遇到的问题之一。<br>当使用基于梯度的优化算法（如反向传播）来更新神经网络的权重时，梯度消失指的是在<strong>网络的较早层（通常是深层）中，梯度逐渐变得非常小，接近于零，甚至完全消失。</strong><br>梯度消失的主要原因是使用某些激活函数（如Sigmoid或Tanh）时，这些函数的导数在输入值非常大或非常小的情况下会趋近于零。当反向传播的梯度通过多个层传递时，梯度值会不断缩小，最终导致较早层的梯度非常小或消失。</p></blockquote><p>而MSE（均方误差）是一种常用的损失函数，用于衡量预测值与目标值之间的差异。MSE本身并不能直接解决梯度消失问题。</p><p>代码示例</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">mean_squared_error</span>(<span class="hljs-params">y_true, y_pred</span>):<br>    <span class="hljs-keyword">return</span> np.mean(np.square(y_pred - y_true), axis=-<span class="hljs-number">1</span>)<br></code></pre></td></tr></table></figure><p>(2) <strong>平均绝对误差损失函数</strong></p><p>平均绝对误差(Mean Absolute Error Loss, MAE)损失函数定义如下：</p><p><img src="/2023/07/13/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/img2.png" alt="mae.png"></p><p>代码示例</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">mean_absolute_error</span>(<span class="hljs-params">y_true, y_pred</span>):<br>    <span class="hljs-keyword">return</span> np.mean(np.<span class="hljs-built_in">abs</span>(y_pred - y_true), axis=-<span class="hljs-number">1</span>)<br></code></pre></td></tr></table></figure><p>(3) <strong>均方误差对数损失函数</strong></p><p>均方误差对数(Mean Squared Log Error Loss, MSLE)损失函数定义如下：<br><img src="/2023/07/13/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/img3.png" alt="msle.png"></p><p>代码示例</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">mean_squared_logarithmic_error</span>(<span class="hljs-params">y_true, y_pred</span>):<br>    first_log = np.log(np.clip(y_pred, <span class="hljs-number">10e-6</span>, <span class="hljs-literal">None</span>) + <span class="hljs-number">1.</span>)<br>    second_log = np.log(np.clip(y_true, <span class="hljs-number">10e-6</span>, <span class="hljs-literal">None</span>) + <span class="hljs-number">1.</span>)<br>    <span class="hljs-keyword">return</span> np.mean(np.square(first_log - second_log), axis=-<span class="hljs-number">1</span>)<br></code></pre></td></tr></table></figure><p>(4) 平均绝对百分比误差损失函数</p><p>平均绝对百分比(Mean Absolute Percentage Error Loss, MAPE)误差损失函数定义如下：<br><img src="/2023/07/13/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/img4.png" alt="mape.png"></p><p>代码示例</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">mean_absolute_percentage_error</span>(<span class="hljs-params">y_true, y_pred</span>):<br>    diff = np.<span class="hljs-built_in">abs</span>((y_pred - y_true) / np.clip(np.<span class="hljs-built_in">abs</span>(y_true), <span class="hljs-number">10e-6</span>, <span class="hljs-literal">None</span>))<br>    <span class="hljs-keyword">return</span> <span class="hljs-number">100</span> * np.mean(diff, axis=-<span class="hljs-number">1</span>)<br></code></pre></td></tr></table></figure><p>(5) 小结<br>    <strong>均方误差损失函数是使用最广泛的</strong>，并且在大部分情况下，均方误差有着不错的性能，因此被用作损失函数的<strong>基本衡量指标。</strong><br>MAE 则会比较有效地惩罚异常值，如果数据异常值较多，需要考虑使用平均绝对误差损失作为损失函数。<br>    一般情况下，为了不让数据出现太多异常值，可以对数据进行预处理操作。 均方误差对数损失与均方误差的计算过程类似，多了对每个输出数据进行对数计算，<strong>目的是缩小函数输出的范围值</strong>。 </p><p>平均绝对百分比误差损失则计算<strong>预测值与真实值的相对误差</strong>。均方误差对数损失与平均绝对百分比误差损失实际上是用来处理<strong>大范围数据</strong>的， 但是在神经网络中，我们常把输入数据归一化到一个合理范围([−1,1])，然后再使用均方误差或者平均绝对误差损失来计算损失。</p><hr><h2 id="5-3-分类损失函数"><a href="#5-3-分类损失函数" class="headerlink" title="5.3 分类损失函数"></a>5.3 分类损失函数</h2><p><strong>这一部分，了解就行了，毕竟我也不会</strong></p><p>(1) Logistic损失函数</p><p>Logistic损失函数定义如下：</p><p><img src="/2023/07/13/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/img5.png" alt="Logistic.png"></p><p>(2) 负对数似然损失函数</p><p>负对数似然损失函数(Negative Log Likelihood Loss)定义如下：</p><p><img src="/2023/07/13/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/img6.png" alt="负对数似然.png"></p><p> (3) 交叉熵损失函数</p><p><strong>这个比较重要 记一下</strong></p><p>Logistic损失函数和负对数似然损失函数只能处理二分类问题，对于两个分类扩展到M个分类，使用交叉熵损失函数(Cross Entropy Loss)，其定义如下：</p><p><img src="/2023/07/13/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/img7.png" alt="交叉熵.png"></p><p> 代码示例</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">cross_entropy</span>(<span class="hljs-params">y_true, y_pred</span>):<br>    <span class="hljs-keyword">return</span> -np.mean(y_true * np.log(y_pred + <span class="hljs-number">10e-6</span>))<br></code></pre></td></tr></table></figure><hr><h2 id="5-4-神经网络中常用的损失函数"><a href="#5-4-神经网络中常用的损失函数" class="headerlink" title="5.4 神经网络中常用的损失函数"></a>5.4 神经网络中常用的损失函数</h2><p>神经网络中的损失函数可以自定义，前提是<strong>需要考虑数据本身和用于求解的优化方案</strong>。</p><p>换句话说，自定义损失函数<strong>需要考虑输入的数据形式和对损失函数求导的算法</strong>。自定义损失函数其实是有些难度的，在实际工程项目上，结合激活函数来选择损失函数是常见的做法，常用组合有以下 3 种 。</p><ul><li><strong>ReLU + MSE</strong></li></ul><p>均方误差损失函数<strong>无法处理梯度消失问题</strong>，而使用 Leak ReLU 激活函数能够减少计算时梯度消失的问题.</p><blockquote><p>因此在神经网络中如果需要使用均方误差损失函数，一般采用 Leak ReLU 等可以减少梯度消失的激活函数。另外，由于均方误差具有普遍性，一般作为衡量损失值的标准，因此使用均方误差作为损失函数表现既不会太好也不至于太差。</p></blockquote><ul><li><strong>Sigmoid + Logistic</strong></li></ul><p>Sigmoid 函数会引起<strong>梯度消失问题</strong></p><blockquote><p>根据链式求导法，Sigmoid 函数求导后由多个［0, 1］范围的数进行连乘，如其导数形式为 ，当其中一个数很小时，连成后会无限趋近于零直至最后消失。而类 Logistic 损失函数求导时，加上对数后连乘操作转化为求和操作，在一定程度上避免了梯度消失，所以我们经常可以看到 Sigmoid 激活函数＋交叉摘损失函数 的组合。</p></blockquote><ul><li><strong>Softmax + Logisitc</strong></li></ul><p>在数学上，Softmax 激活函数会<strong>返回输出类的互斥概率分布</strong>，也就是能把离散的输出转换为一个同分布互斥的概率，如(0.2, 0.8)。</p><blockquote><p>另外，Logisitc 损失函数是基于概率的最大似然估计函数而来的，因此输出概率化能够更加方便优化算法进行求导和计算，所以我们经常可以看到输出层使xu用 Softmax 激活函数＋交叉熵损失函数 的组合。</p></blockquote><h2 id="5-5-激活函数、损失函数、优化函数的区别"><a href="#5-5-激活函数、损失函数、优化函数的区别" class="headerlink" title="5.5 激活函数、损失函数、优化函数的区别"></a>5.5 激活函数、损失函数、优化函数的区别</h2><ol><li><p><strong>激活函数</strong>：将神经网络上一层的输入，经过神经网络层的非线性变换转换后，通过激活函数，得到输出。常见的激活函数包括：sigmoid, tanh, relu等。</p></li><li><p><strong>损失函数</strong>：度量神经网络的输出的预测值，与实际值之间的差距的一种方式。常见的损失函数包括：最小二乘损失函数、交叉熵损失函数、回归中使用的smooth L1损失函数等。</p></li><li><p><strong>优化函数</strong>：也就是如何把损失值从神经网络的最外层传递到最前面。如最基础的梯度下降算法，随机梯度下降算法，批量梯度下降算法，带动量的梯度下降算法，Adagrad，Adadelta，Adam等。</p></li></ol>]]></content>
    
    
    
    <tags>
      
      <tag>CV</tag>
      
      <tag>神经网络</tag>
      
      <tag>深度学习</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Hadoop ——MapReduce 详解</title>
    <link href="/2023/07/12/Hadoop-%E2%80%94%E2%80%94MapReduce-%E8%AF%A6%E8%A7%A3/"/>
    <url>/2023/07/12/Hadoop-%E2%80%94%E2%80%94MapReduce-%E8%AF%A6%E8%A7%A3/</url>
    
    <content type="html"><![CDATA[]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>Hadoop 原理简介</title>
    <link href="/2023/07/12/Hadoop%E5%8E%9F%E7%90%86%E7%AE%80%E4%BB%8B/"/>
    <url>/2023/07/12/Hadoop%E5%8E%9F%E7%90%86%E7%AE%80%E4%BB%8B/</url>
    
    <content type="html"><![CDATA[<h1 id="一、Hadoop分类及架构"><a href="#一、Hadoop分类及架构" class="headerlink" title="一、Hadoop分类及架构"></a>一、Hadoop分类及架构</h1><h2 id="1-1-Hadoop分类"><a href="#1-1-Hadoop分类" class="headerlink" title="1.1 Hadoop分类"></a>1.1 Hadoop分类</h2><p>Hadoop 是一个适合大数据的分布式存储和计算平台。</p><p>狭义上说Hadoop就是一个框架平台，广义上讲Hadoop代表大数据的一个技术生态圈，包括很多其他软件框架。狭义的又分1.x版本和2.x版本，当然还有后面的3.x版本。</p><p>本文将会从最简单的1.x开始探讨，然后逐步讨论2.x版本以及Hadoop生态圈。</p><h2 id="1-2-Hadoop核心架构"><a href="#1-2-Hadoop核心架构" class="headerlink" title="1.2 Hadoop核心架构"></a>1.2 Hadoop核心架构</h2><p>首先关于狭义的1.x版本。</p><p><strong>Hadoop&#x3D;Hbase+MapReduce+HDFS</strong></p><p>Hbase：实时分布式数据库</p><p>MapReduce：分布式计算框架</p><p>HDFS：分布式文件系统</p><p>其中，HDFS和MapReduce是Hadoop<em><strong>最重要的两个组件（无论哪个版本）</strong></em>，因此，本文将重点讨论两者原理。</p><hr><h1 id="二、Hadoop核心部分工作原理"><a href="#二、Hadoop核心部分工作原理" class="headerlink" title="二、Hadoop核心部分工作原理"></a>二、Hadoop核心部分工作原理</h1><h2 id="2-1-HDFS原理"><a href="#2-1-HDFS原理" class="headerlink" title="2.1 HDFS原理"></a>2.1 HDFS原理</h2><h3 id="2-1-1-HDFS简介"><a href="#2-1-1-HDFS简介" class="headerlink" title="2.1.1 HDFS简介"></a>2.1.1 HDFS简介</h3><p>HDFS是典型的主从式架构，采用TCP&#x2F;IP通信，具体架构图如下：</p><p><img src="/2023/07/12/Hadoop%E5%8E%9F%E7%90%86%E7%AE%80%E4%BB%8B/image.png" alt="架构图"></p><p><strong>NameNode</strong>：是Master节点（主节点），可以看作是分布式文件系统中的<strong>管理者</strong>，负责管理文件系统的命名空间、集群配置信息和存储块的复制等。包括了文件信息、每一个文件对应的文件块的信息和每一个文件块在DataNode的信息等。</p><p><strong>DataNode</strong>：是Slave节点（从节点），<strong>文件存储的基本单元</strong>，它将Block存储在本地文件系统中，保存了Block的Meta-data，同时周期性地将所有存在的Block信息发送给NameNode。</p><p><strong>Client</strong>：切分文件；访问HDFS；与NameNode交互，获得文件位置信息；与DataNode交互，读取和写入数据。 </p><p><strong>Block</strong>：HDFS中的基本读写单元；HDFS中的文件都是被切割为block（块）进行存储的；这些块被复制到多个DataNode中；块的大小（通常为64MB）和复制的块数量在创建文件时由Client决定。</p><hr><h3 id="2-1-2-HDFS-写入流程"><a href="#2-1-2-HDFS-写入流程" class="headerlink" title="2.1.2 HDFS 写入流程"></a>2.1.2 HDFS 写入流程</h3><p><img src="/2023/07/12/Hadoop%E5%8E%9F%E7%90%86%E7%AE%80%E4%BB%8B/image-1.jpg" alt="写入流程"></p><p><strong>请注意！作为大数据平台搭建者，身份应该是服务者server而非client</strong></p><ol><li><p>用户向Client（客户机）提出请求。例如，需要写入200MB的数据。</p></li><li><p>Client制定计划：将数据按照64MB为块，进行切割；所有的块都保存三份。</p><blockquote><p><em>HDFS具有<strong>高容错性</strong>的特点，通过增加副本的形式，提高容错性。某一个副本丢失以后，可以自动恢复</em></p></blockquote></li><li><p>Client将大文件切分成块（block）</p><blockquote><p><em>HDFS<strong>适合处理大数据</strong>，能够处理数据规模达到GB\TB、甚至PB级别的数据</em></p></blockquote></li><li><p>针对第一个块，Client告诉NameNode（主控节点），请帮助我，将64MB的块复制三份.</p></li><li><p>NameNode告诉Client三个DataNode（数据节点）的地址，并且将它们根据到Client的距离，进行了排序。</p></li><li><p>Client把数据和清单发给第一个DataNode。</p></li><li><p>第一个DataNode将数据复制给第二个DataNode。</p><blockquote><p><em>不适合<strong>低延时数据</strong>访问，比如毫秒级的存储数据，是做不到的</em><br><em>无法高效的对大量小文件进行<strong>存储</strong></em></p></blockquote></li><li><p>第二个DataNode将数据复制给第三个DataNode。</p></li><li><p>如果某一个块的所有数据都已写入，就会向NameNode反馈已完成。</p></li><li><p>对第二个Block，也进行相同的操作。</p><blockquote><p><em>不支持并发写入、文件随机修改。仅支持append（追加）</em></p></blockquote></li><li><p>所有Block都完成后，关闭文件。NameNode会将数据持久化到磁盘上。</p></li></ol><hr><h3 id="2-1-3-HDFS-读取流程"><a href="#2-1-3-HDFS-读取流程" class="headerlink" title="2.1.3 HDFS 读取流程"></a>2.1.3 HDFS 读取流程</h3><p><img src="/2023/07/12/Hadoop%E5%8E%9F%E7%90%86%E7%AE%80%E4%BB%8B/image-2.png" alt="读取流程"></p><ol><li><p>用户向Client提出读取请求。</p></li><li><p>Client向NameNode请求这个文件的所有信息。</p></li><li><p>NameNode将给Client这个文件的块列表，以及存储各个块的数据节点清单（按照和客户端的距离排序）。</p></li><li><p>Client从距离最近的数据节点下载所需的块。</p></li></ol><p><strong>适合一次读入、多次读出的场景，且不支持文件的修改。适合用来做数据分析，非不是网盘应用</strong></p><hr><h2 id="2-2-MapReduce原理"><a href="#2-2-MapReduce原理" class="headerlink" title="2.2 MapReduce原理"></a>2.2 MapReduce原理</h2><h3 id="2-2-1-MapReduce-流程"><a href="#2-2-1-MapReduce-流程" class="headerlink" title="2.2.1 MapReduce 流程"></a>2.2.1 MapReduce 流程</h3><p>再来看看MapReduce</p><p>MapReduce实则是由两个部分构成的，<strong>Map（映射）</strong>和<strong>Reduce（归约）</strong><br>可以说，前者负责”分“，后者负责”合“，这两个机制可以有效帮助不会并行运算的程序员实现并行技术，是一项可靠的，具有容错能力的技术。<br>这个定义中包含5个关键词：</p><ul><li><strong>软件框架</strong></li><li><strong>并行处理</strong></li><li><strong>可靠且容错</strong></li><li><strong>大规模集群</strong></li><li><strong>海量数据集</strong></li></ul><p>当我们向MapReduce提交一个计算作业时，它会首先把计算作业拆分成若干个<strong>Map任务</strong>，然后分配到不同的节点上执行，每一个Map任务处理输入数据中的一部分，当Map任务完成后，<br>它会生成一些中间文件，这些中间文件将会作为<strong>Reduce任务</strong>的输入数据。<br>Reduce任务的主要目标就是<strong>把前面若干个Map的输出汇总到一起并输出。</strong></p><p><strong>请注意！大数据技术解决的主要是海量数据的存储和计算，技术任务中通常包含计算，这与HDFS往往是一同进行的，也就是说，文件传输到Hadoop平台中，会先使用HDFS技术进行存储、切块和备份，正如上文所讲。然后再进行MapReduce计算。这也意味着，MapReduce的input来源于HDFS。</strong></p><p>具体架构图如下：</p><p><img src="/2023/07/12/Hadoop%E5%8E%9F%E7%90%86%E7%AE%80%E4%BB%8B/img.png" alt="MapReduce架构图"></p><p>看来很抽象，举个例子，比如我们需要一个统计文本中的词频工作，其工作图长这样：</p><p><img src="/2023/07/12/Hadoop%E5%8E%9F%E7%90%86%E7%AE%80%E4%BB%8B/img-1.png" alt="词频分析.png"></p><ol><li><p>首先，Hadoop会将输入的数据切成若干个分片，并将每一个分片交给一个map task（Map任务）处理</p><blockquote><p><em>注意，这里的<strong>split</strong>和前文的HDFS中的<strong>block</strong>有极大的不同，split是一个<strong>逻辑概念</strong>，block是一个<strong>物理概念</strong>，默认切片大小与block块相同，因为如果超过block块，可能会产生跨节点的网络io问题</em></p></blockquote></li><li><p>Mapping之后，相当于得到了每一个task里面，每个词以及它出现的次数</p></li><li><p>shuffle（拖移）将相同的词放在一起，并对它们进行排序，分成若干个分片。</p><blockquote><p><em>Map和shuffle概念较为复杂，此处并不详说。详细可看<a href="Hadoop-%E2%80%94%E2%80%94MapReduce-%E8%AF%A6%E8%A7%A3.md">Hadoop之MapReduce 详解</a></em></p></blockquote></li><li><p>根据这些分片，进行reduce（归约）。</p><blockquote><p><em>注意，Reducing阶段中，每一个key调用一次reduce方法，Reduce个数是自己指定的，一个reduce对应一个输出文件</em></p></blockquote></li><li><p>统计出reduce task的结果，输出到文件。</p><blockquote><p><em>正如前文所言，MapReduce是依托HDFS的，所以把数据输出到HDFS上</em></p></blockquote></li></ol><p><strong>简单来说，MapReduce这个框架模型，极大地方便了编程人员在不会分布式并行编程的情况下，将自己的程序运行在分布式系统上</strong></p><hr><h3 id="2-2-2-MapReduce角色"><a href="#2-2-2-MapReduce角色" class="headerlink" title="2.2.2 MapReduce角色"></a>2.2.2 MapReduce角色</h3><p>为了完成上述流程。在MapReduce里，拥有两个角色进行辅助：<strong>JobTracker</strong>以及<strong>TaskTracker</strong><br>严格意义上讲，MapReduce的操作流程是这样的：</p><p><img src="/2023/07/12/Hadoop%E5%8E%9F%E7%90%86%E7%AE%80%E4%BB%8B/img-2.png" alt="img.png"></p><ul><li><strong>JobTracker</strong>：<br>负责调度构成一个作业的所有任务，这些任务分布在不同的TaskTracker上（由上图的JobTracker可以看到2 assign map 和 3 assign reduce）。你可以将其理解为公司的项目经理，项目经理接受项目需求，并划分具体的任务给下面的开发工程师。</li><li><strong>TaskTracker</strong>：<br>TaskTracker负责执行由JobTracker指派的任务，这里我们就可以将其理解为开发工程师，完成项目经理安排的开发任务即可。</li></ul><p><strong>从原理上说，JobTracker从client得到任务，将任务调度给TaskTracker上，然后去client交回任务</strong></p><hr><h1 id="三、Hadoop-1-x版本和2-x版本类同"><a href="#三、Hadoop-1-x版本和2-x版本类同" class="headerlink" title="三、Hadoop 1.x版本和2.x版本类同"></a>三、Hadoop 1.x版本和2.x版本类同</h1><h2 id="2-1-Hadoop的2-x版本"><a href="#2-1-Hadoop的2-x版本" class="headerlink" title="2.1 Hadoop的2.x版本"></a>2.1 Hadoop的2.x版本</h2><p>2011年11月，Hadoop 1.0.0版本正式发布，但存在很多问题：  </p><ol><li>扩展性差，JobTracker负载较重，成为性能瓶颈。</li><li>可靠性差，NameNode只有一个，万一挂掉，整个系统就会崩溃。</li><li>仅适用MapReduce一种计算方式。</li><li>资源管理的效率比较低。</li></ol><p>所以，2012年5月，Hadoop推出了2.0版本，在原本的分工中，MapReduce负责资源管理和计算。在2.x中，在HDFS之上，增加了<strong>YARN（资源管理框架）层</strong>。它是一个资源管理模块，为各类应用程序提供资源管理和调度。</p><p><img src="/2023/07/12/Hadoop%E5%8E%9F%E7%90%86%E7%AE%80%E4%BB%8B/img-3.png" alt="Hadoop 1.0与2.0对比"></p><h2 id="2-2-Yarn原理"><a href="#2-2-Yarn原理" class="headerlink" title="2.2 Yarn原理"></a>2.2 Yarn原理</h2><h3 id="2-2-1-Yarn简介"><a href="#2-2-1-Yarn简介" class="headerlink" title="2.2.1 Yarn简介"></a>2.2.1 Yarn简介</h3><p>前文提到MapReduce是依托HDFS而生的，同样，Yarn也是依托HDFS。在功能上说，Yarn和MapReduce是不可能分开的。</p><p>Yarn也是主从式架构，主节点为ResourceManager，从节点NodeManager。其功能架构图如下：</p><p><img src="/2023/07/12/Hadoop%E5%8E%9F%E7%90%86%E7%AE%80%E4%BB%8B/img-6.png" alt="Yarn功能架构图"></p><ul><li><p><strong>ResourceManager</strong></p><blockquote><p>全局资源管理器由两部分组成，分别为Scheduler 和 ApplicationsManager。</p></blockquote><ol><li><p><strong>ApplicationsManager</strong>  </p><blockquote><p>主要负责为应用（如 mapreduce程序）分配第一个Container（资源池）来运行我们的ApplicationsMaster；负责监控ApplicationMaster，并且在遇到失败的时候重启ApplicationMaster。</p></blockquote></li><li><p><strong>Scheduler</strong></p><blockquote><p>调度器负责将资源分配给各种正在运行的应用程序，这些应用程序受到容量、队列等常见的限制。调度器是纯粹的调度器，它不监视或跟踪应用程序的状态。此外，它也不能保证重启失败的应用程（由ApplicationManager负责）。调度器根据应用程序的资源需求执行其调度功能；它是基于Container的抽象概念来实现资源调度的，Scheduler通过给任务分配Container的方式来分配资源。</p></blockquote></li></ol><blockquote><p>调度器是可插拔的，常用的调度器有CapacityScheduler和FairScheduler。</p></blockquote></li><li><p><strong>NodeManager</strong></p><blockquote><p>NodeManager节点资源管理器，是每台机器的框架客户端&#x2F;代理，负责Container容器管理，监控他们的资源使用，并汇报给ResourceManager&#x2F;Scheduler。</p></blockquote><ol><li><strong>Container</strong>：<blockquote><p>NodeManager将节点中的资源切分出来组成一个个可以单独运行任务（map，reduce）的Container容器，用来运行任务。容器中资源目前只支内存、cpu。一个Container类似于开辟了一个虚拟机，一个NodeManager中可以分配一个或者多个Container。Container底层使用了轻量级资源隔离机制Cgroups进行资源隔离（docker底层好像也是使用了Cgroups）。</p></blockquote></li></ol></li></ul><p><strong>Container是Yarn集群分配资源的基本单位</strong>。</p><ul><li><strong>ApplicationMaster</strong><blockquote><p>ApplicationMaster运行在NodeManager的Container中，并且是应用程序申请到的第一个Container。它负责一个应用程序中具体任务的执行，比如mapreduce应用的map任务，reduce任务，每一个应用都有一个ApplicationMaster，它受ApplicationsManager管理，ApplicationMaster进程如果非正常死亡，ApplicationsManager可以重启它。<br>主要功能：</p></blockquote><ol><li>与ResourceManager中的Scheduler 协商获取执行资源。</li><li>与NodeManager通讯以启动，停止任务</li><li>监控所属的任务的执行。</li></ol></li></ul><h1 id="四、Hadoop生态圈"><a href="#四、Hadoop生态圈" class="headerlink" title="四、Hadoop生态圈"></a>四、Hadoop生态圈</h1><h2 id="4-1-Hadoop-生态圈"><a href="#4-1-Hadoop-生态圈" class="headerlink" title="4.1 Hadoop 生态圈"></a>4.1 Hadoop 生态圈</h2><p><img src="/2023/07/12/Hadoop%E5%8E%9F%E7%90%86%E7%AE%80%E4%BB%8B/img-10.png" alt="词频分析.png"></p><p>在整个Hadoop架构中，计算框架起到<strong>承上启下</strong>的作用，一方面可以操作HDFS中的数据，另一方面可以被封装，提供Hive、Pig这样的上层组件的调用。</p><p>简单介绍一下其中几个比较重要的组件。</p><p><strong>HBase</strong>：来源于Google的BigTable；是一个高可靠性、高性能、面向列、可伸缩的<strong>分布式数据库</strong>。</p><p><strong>Hiv</strong>e：是一个数据仓库工具，可以将结构化的数据文件映射为一张数据库表，通过类SQL语句快速实现简单的MapReduce统计，不必开发专门的MapReduce应用，十分适合数据仓库的统计分析。</p><p><strong>Pig</strong>：是一个基于Hadoop的大规模数据分析工具，它提供的SQL-LIKE语言叫Pig Latin，该语言的编译器会把类SQL的数据分析请求转换为一系列经过优化处理的MapReduce运算。</p><p><strong>ZooKeeper</strong>：来源于Google的Chubby；它主要是用来解决分布式应用中经常遇到的一些数据管理问题，简化分布式应用协调及其管理的难度。</p><p><strong>Ambari</strong>：Hadoop管理工具，可以快捷地监控、部署、管理集群。</p><p><strong>Sqoop</strong>：用于在Hadoop与传统的数据库间进行数据的传递。</p><p><strong>Mahout</strong>：一个可扩展的机器学习和数据挖掘库。  </p><p>如果还是有点疑惑的话，这张图应当会较为直观：</p><p><img src="/2023/07/12/Hadoop%E5%8E%9F%E7%90%86%E7%AE%80%E4%BB%8B/img-4.png" alt="Hadoop生态群.png"></p><p><strong>总的来看，Hadoop有以下优点：</strong></p><ul><li><strong>高可靠性</strong>：这个是由它的基因决定的。它的基因来自Google。Google最擅长的事情，就是“垃圾利用”。Google起家的时候就是穷，买不起高端服务器，所以，特别喜欢在普通电脑上部署这种大型系统。虽然硬件不可靠，但是系统非常可靠。</li><li><strong>高扩展性</strong>：Hadoop是在可用的计算机集群间分配数据并完成计算任务的，这些集群可以方便地进行扩展。说白了，想变大很容易。</li><li><strong>高效性</strong>：Hadoop能够在节点之间动态地移动数据，并保证各个节点的动态平衡，因此处理速度非常快。</li><li><strong>高容错性</strong>：Hadoop能够自动保存数据的多个副本，并且能够自动将失败的任务重新分配。这个其实也算是高可靠性。</li><li><strong>低成本</strong>：Hadoop是开源的，依赖于社区服务，使用成本比较低。</li></ul><p>基于这些优点，Hadoop适合应用于大数据存储和大数据分析的应用，适合于服务器几千台到几万台的集群运行，支持PB级的存储容量。</p><p>Hadoop的应用非常广泛，包括：<strong>搜索、日志处理、推荐系统、数据分析、视频图像分析、数据保存</strong>等，都可以使用它进行部署</p><h1 id="五、Spark简介"><a href="#五、Spark简介" class="headerlink" title="五、Spark简介"></a>五、Spark简介</h1><p>最后，再说说Spark。</p><p><img src="/2023/07/12/Hadoop%E5%8E%9F%E7%90%86%E7%AE%80%E4%BB%8B/img_5.png" alt="Spark.png"></p><p>Spark同样是Apache软件基金会的顶级项目。它可以理解为在Hadoop基础上的一种改进。 它是加州大学伯克利分校AMP<br>实验室所开源的类Hadoop MapReduce的通用并行框架。相对比Hadoop，它可以说是青出于蓝而胜于蓝。</p><p>正如前文所言，MapReduce是面向<strong>磁盘</strong>的。因此，受限于磁盘读写性能的约束，MapReduce在处理迭代计算、实时计算、交互式数据查询等方面并不高效<br>。但是，这些计算却在图计算、数据挖掘和机器学习等相关应用领域中非常常见。</p><p>而Spark是面向<strong>内存</strong>的。这使得Spark能够为多个不同数据源的数据提供近乎<strong>实时</strong>的处理性能，适用于需要多次操作特定数据集的应用场景。</p><p>在相同的实验环境下处理相同的数据，若在内存中运行，那么Spark要比MapReduce快100倍。<br><strong>其它方面，例如处理迭代运算、计算数据分析类报表、排序等，Spark都比MapReduce快很多。</strong></p><p>此外，Spark在易用性、通用性等方面，也比Hadoop更强。</p>]]></content>
    
    
    
    <tags>
      
      <tag>Hadoop</tag>
      
      <tag>大数据</tag>
      
      <tag>实习</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>搭建一个基于爬虫功能图像三分类系统</title>
    <link href="/2023/07/12/%E6%90%AD%E5%BB%BA%E5%9F%BA%E4%BA%8E%E7%88%AC%E8%99%AB%E6%8A%80%E6%9C%AF%E7%9A%84%E5%9B%BE%E5%83%8F%E4%B8%89%E5%88%86%E7%B1%BB%E7%B3%BB%E7%BB%9F/"/>
    <url>/2023/07/12/%E6%90%AD%E5%BB%BA%E5%9F%BA%E4%BA%8E%E7%88%AC%E8%99%AB%E6%8A%80%E6%9C%AF%E7%9A%84%E5%9B%BE%E5%83%8F%E4%B8%89%E5%88%86%E7%B1%BB%E7%B3%BB%E7%BB%9F/</url>
    
    <content type="html"><![CDATA[<h1 id="一、功能分析"><a href="#一、功能分析" class="headerlink" title="一、功能分析"></a>一、功能分析</h1><p>整个任务就分成两个大类。一是爬虫技术，二是图像识别。<br>首先是爬虫技术，为了满足其后图像识别的三分类效果，应当设计一个可根据关键词进行搜索的功能。因而最佳的爬取网站就是百度。<br>其次是图像识别，本报告着重应用其图像分类功能。通过搭建神经网络，与划分数据集的方式进行识别。并做一个简单的训练可视化。<br>在确定了根据关键词爬取百度图片+神经网络三分类的基本思路后。再来确定关键的模块化与GUI界面。<br>首先关于模块化、多线程等技术，则任务需要分程序实现，应当有一个主函数以及多个负责爬虫、神经网络识别、GUI窗口的模块。<br>其次是关于图形界面的问题，即GUI窗口。可以使用tkinter包来实现这以功能。为了保证整体美观的效果。也可考虑对GUI窗口进行一定程度上的设计，包括增添背景图片、添加文字说明等。<br>根据上述分析，本任务主要需求为：<br>1.搭建一个可以基于用户的三个关键词进行爬取的爬虫系统。并且设计出有关爬虫的GUI界面；<br>2.归纳整理爬取的数据为数据集。设计程序进行测试集与验证集划分；<br>3.设计一个神经网络进行图像识别分类任务。同时写train与test函数进行验证与训练。可保留训练日志；<br>4.设计一个图像识别相关界面，进行训练可视化。</p><hr><h1 id="二、设计方案与具体细节"><a href="#二、设计方案与具体细节" class="headerlink" title="二、设计方案与具体细节"></a>二、设计方案与具体细节</h1><h2 id="2-1-设计用于爬虫程序读取数据的GUI窗口"><a href="#2-1-设计用于爬虫程序读取数据的GUI窗口" class="headerlink" title="2.1  设计用于爬虫程序读取数据的GUI窗口"></a>2.1  设计用于爬虫程序读取数据的GUI窗口</h2><p>针对爬取图片这一问题，首先我们定义一个图形窗口用来获得用户需求。基于此，我使用python的tkinter包达成这一目的。为了达到我们预想中的GUI界面实现效果，首先要明确的是，为了更方便的从界面中获取数据值，所以我们的变量应当都是全局变量方便调用。基于此，我们分成以下步骤进行实现框架。</p><hr><p>1.在窗口的设计方面；新建Img文件夹，可以制作.ico后缀照片作为窗口样式，背景上可以通过tkinter中的ImageTK.PhotoImage引入自己想要的图片，然后使用place进行放置。<br>‘’’iconPath &#x3D; ‘..\Img\favicon.ico’<br>    window &#x3D; Tk()<br>    window.title(‘python爬虫读取页面’)<br>    image2 &#x3D; Image.open(r’Img\background (2).gif’)<br>    background_image &#x3D; ImageTk.PhotoImage(image2)<br>    w &#x3D; background_image.width()<br>    h &#x3D; background_image.height()<br>    window.geometry(‘%dx%d+0+0’ % (w, h))<br>    background_label &#x3D; Label(window, image&#x3D;background_image)<br>    background_label.place(x&#x3D;0, y&#x3D;0, relwidth&#x3D;1, relheight&#x3D;1)<br>    window.iconbitmap(iconPath)’’’</p><hr><p>2.在窗口的部件设置上；首先必要的是文字提示语与输入框。包括关键词与照片数量、存储文件夹名，可以通过label标签以及Entry输入来实现这一功能。其次是显示爬虫进度的小窗，包括显示爬取的照片数量以及图片地址，可以通过引入treeview这一方法，同时设计column顶部与scrollbar侧边滑动条来使得界面更加美观。最后是启动按钮与退出按钮，可以直接使用button标签来达到这一功能。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python">Label(window, text=<span class="hljs-string">&quot;请输入想爬取图片关键词1：&quot;</span>, font=(<span class="hljs-string">&quot;微软雅黑&quot;</span>, <span class="hljs-number">15</span>), fg=<span class="hljs-string">&quot;black&quot;</span>,bg=<span class="hljs-string">&quot;pink&quot;</span>).place(x=<span class="hljs-number">205</span>, y=<span class="hljs-number">0</span>, anchor=NW)<br>label1_input = Entry(window, font=(<span class="hljs-string">&quot;微软雅黑&quot;</span>, <span class="hljs-number">15</span>), width=<span class="hljs-number">5</span>)<br>label1_input.place(x=<span class="hljs-number">455</span>, y=<span class="hljs-number">0</span>)<br>Label(window, text=<span class="hljs-string">&quot;请输入想爬取的图片个数：&quot;</span>, font=(<span class="hljs-string">&quot;微软雅黑&quot;</span>, <span class="hljs-number">15</span>), fg=<span class="hljs-string">&quot;black&quot;</span>, bg=<span class="hljs-string">&quot;pink&quot;</span>).place(x=<span class="hljs-number">525</span>, y=<span class="hljs-number">0</span>)<br>data1_input = Entry(window, font=(<span class="hljs-string">&quot;微软雅黑&quot;</span>, <span class="hljs-number">15</span>), width=<span class="hljs-number">5</span>)<br>data1_input.place(x=<span class="hljs-number">765</span>, y=<span class="hljs-number">0</span>)<br>Label(window, text=<span class="hljs-string">&quot;请输入存储文件夹：&quot;</span>, font=(<span class="hljs-string">&quot;微软雅黑&quot;</span>, <span class="hljs-number">15</span>), fg=<span class="hljs-string">&quot;black&quot;</span>, bg=<span class="hljs-string">&quot;pink&quot;</span>).place(x=<span class="hljs-number">835</span>, y=<span class="hljs-number">0</span>)<br>location1_input = Entry(window, font=(<span class="hljs-string">&quot;微软雅黑&quot;</span>, <span class="hljs-number">15</span>), width=<span class="hljs-number">5</span>)<br>location1_input.place(x=<span class="hljs-number">1020</span>, y=<span class="hljs-number">0</span>)<br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs python">treeview_width = w - <span class="hljs-number">40</span><br>treeview_height = h / <span class="hljs-number">2</span> - <span class="hljs-number">30</span><br><span class="hljs-keyword">global</span> tree<br>tree = ttk.Treeview(window, height=<span class="hljs-number">10</span>, show=<span class="hljs-string">&#x27;headings&#x27;</span>, selectmode=<span class="hljs-string">&#x27;browse&#x27;</span>,<br>                        takefocus=<span class="hljs-literal">True</span>,columns=(<span class="hljs-string">&quot;totalCount&quot;</span>, <span class="hljs-string">&quot;Name&quot;</span>))<br>tree.column(<span class="hljs-string">&quot;totalCount&quot;</span>, width=<span class="hljs-built_in">int</span>(treeview_width * <span class="hljs-number">0.1</span>))<br>tree.column(<span class="hljs-string">&quot;Name&quot;</span>, width=<span class="hljs-built_in">int</span>(treeview_width * <span class="hljs-number">0.9</span>))<br>tree.heading(<span class="hljs-string">&quot;totalCount&quot;</span>, text=<span class="hljs-string">&quot;序号&quot;</span>,command=<span class="hljs-keyword">lambda</span> c=<span class="hljs-string">&quot;totalCount&quot;</span>:treeview_sort_column(tree, c, <span class="hljs-literal">False</span>, <span class="hljs-string">&#x27;float&#x27;</span>))<br>tree.heading(<span class="hljs-string">&quot;Name&quot;</span>, text=<span class="hljs-string">&quot;图片地址&quot;</span>, command=<span class="hljs-keyword">lambda</span> c=<span class="hljs-string">&quot;Name&quot;</span>:treeview_sort_column(tree, c, <span class="hljs-literal">False</span>, <span class="hljs-string">&#x27;str&#x27;</span>))<br>yscroll = Scrollbar(tree, orient=VERTICAL)<br>yscroll[<span class="hljs-string">&#x27;command&#x27;</span>] = tree.yview<br>yscroll.pack(side=RIGHT, fill=BOTH)<br>tree[<span class="hljs-string">&#x27;yscrollcommand&#x27;</span>] = yscroll.<span class="hljs-built_in">set</span><br>tree.place(x=<span class="hljs-number">10</span>, y=<span class="hljs-number">20</span> + h / <span class="hljs-number">2</span>, width=treeview_width, height=treeview_height)<br></code></pre></td></tr></table></figure><hr><p>3.“开始下载”与“结束”的按钮设计.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python">button1 = Button(window, text=<span class="hljs-string">&quot;开始下载&quot;</span>, font=(<span class="hljs-string">&quot;微软雅黑&quot;</span>, <span class="hljs-number">15</span>),<br>                     command=crawler_content).place(x=<span class="hljs-number">580</span>, y=<span class="hljs-number">95</span>)<br>button2 = Button(window, text=<span class="hljs-string">&quot;分类（请运行network程序）&quot;</span>, font=(<span class="hljs-string">&quot;微软雅黑&quot;</span>, <span class="hljs-number">15</span>),<br>                     command=window.quit).place(x=<span class="hljs-number">500</span>, y=<span class="hljs-number">145</span>)<br>window.mainloop()<br></code></pre></td></tr></table></figure><h2 id="2-2-爬虫主函数实现"><a href="#2-2-爬虫主函数实现" class="headerlink" title="2.2 爬虫主函数实现"></a>2.2 爬虫主函数实现</h2><p>上文的button1的(<code>commend= crawler_content</code>)这一功能实际上是在执行名为crawler_content的函数。这是爬虫实现的主函数，主要功能是得到GUI窗口的关键词、图片数量、文件夹名字等数据。并且运行“crawler”模块实现下载和获取推荐词的功能。  </p>]]></content>
    
    
    
    <tags>
      
      <tag>CV</tag>
      
      <tag>爬虫</tag>
      
      <tag>GUI</tag>
      
      <tag>神经网络</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Hello World</title>
    <link href="/2023/07/12/hello-world/"/>
    <url>/2023/07/12/hello-world/</url>
    
    <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo new <span class="hljs-string">&quot;My New Post&quot;</span><br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo server<br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo generate<br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo deploy<br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content>
    
    
    
  </entry>
  
  
  
  
</search>
