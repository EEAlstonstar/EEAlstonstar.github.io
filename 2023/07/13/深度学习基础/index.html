

<!DOCTYPE html>
<html lang="zh-CN" data-default-color-scheme=auto>



<head>
  <meta charset="UTF-8">
  <link rel="apple-touch-icon" sizes="76x76" href="/img/fluid.jpg">
  <link rel="icon" href="/img/fluid.jpg">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="author" content="yuanxi">
  <meta name="keywords" content="">
  
    <meta name="description" content="暑假打算发篇论文，简单复习一下深度学习基础概念。本片包括感知机、超参数、反向传播、激活函数、损失函数。">
<meta property="og:type" content="article">
<meta property="og:title" content="深度学习基础(上)">
<meta property="og:url" content="http://example.com/2023/07/13/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/index.html">
<meta property="og:site_name" content="yuanxi">
<meta property="og:description" content="暑假打算发篇论文，简单复习一下深度学习基础概念。本片包括感知机、超参数、反向传播、激活函数、损失函数。">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://example.com/2023/07/13/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/img.png">
<meta property="og:image" content="http://example.com/2023/07/13/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/img_1.png">
<meta property="og:image" content="http://example.com/2023/07/13/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/img_2.png">
<meta property="og:image" content="http://example.com/2023/07/13/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/img_3.png">
<meta property="og:image" content="http://example.com/2023/07/13/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/img_4.png">
<meta property="og:image" content="http://example.com/2023/07/13/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/img_5.png">
<meta property="og:image" content="http://example.com/2023/07/13/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/img_6.png">
<meta property="og:image" content="http://example.com/2023/07/13/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/img_7.png">
<meta property="og:image" content="http://example.com/2023/07/13/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/img_8.png">
<meta property="og:image" content="http://example.com/2023/07/13/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/img_9.png">
<meta property="og:image" content="http://example.com/2023/07/13/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/img_10.png">
<meta property="og:image" content="http://example.com/2023/07/13/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/img_11.png">
<meta property="og:image" content="http://example.com/2023/07/13/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/img_12.png">
<meta property="og:image" content="http://example.com/2023/07/13/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/img_13.png">
<meta property="og:image" content="http://example.com/2023/07/13/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/img_14.png">
<meta property="og:image" content="http://example.com/2023/07/13/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/img_15.png">
<meta property="og:image" content="http://example.com/2023/07/13/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/img_16.png">
<meta property="og:image" content="http://example.com/2023/07/13/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/img_17.png">
<meta property="og:image" content="http://example.com/2023/07/13/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/img0.png">
<meta property="og:image" content="http://example.com/2023/07/13/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/img1.png">
<meta property="og:image" content="http://example.com/2023/07/13/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/img2.png">
<meta property="og:image" content="http://example.com/2023/07/13/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/img3.png">
<meta property="og:image" content="http://example.com/2023/07/13/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/img4.png">
<meta property="og:image" content="http://example.com/2023/07/13/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/img5.png">
<meta property="og:image" content="http://example.com/2023/07/13/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/img6.png">
<meta property="og:image" content="http://example.com/2023/07/13/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/img7.png">
<meta property="article:published_time" content="2023-07-13T01:30:14.000Z">
<meta property="article:modified_time" content="2023-07-14T02:45:21.055Z">
<meta property="article:author" content="John Doe">
<meta property="article:tag" content="CV">
<meta property="article:tag" content="神经网络">
<meta property="article:tag" content="深度学习">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="http://example.com/2023/07/13/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/img.png">
  
  
    <meta name="referrer" content="no-referrer-when-downgrade">
  
  
  <title>深度学习基础(上) - yuanxi</title>

  <link  rel="stylesheet" href="https://lib.baomitu.com/twitter-bootstrap/4.6.1/css/bootstrap.min.css" />



  <link  rel="stylesheet" href="https://lib.baomitu.com/github-markdown-css/4.0.0/github-markdown.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/hint.css/2.7.0/hint.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css" />



<!-- 主题依赖的图标库，不要自行修改 -->
<!-- Do not modify the link that theme dependent icons -->

<link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_hj8rtnfg7um.css">



<link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_lbnruvf0jn.css">


<link  rel="stylesheet" href="/css/main.css" />


  <link id="highlight-css" rel="stylesheet" href="/css/highlight.css" />
  
    <link id="highlight-css-dark" rel="stylesheet" href="/css/highlight-dark.css" />
  




  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    Fluid.ctx = Object.assign({}, Fluid.ctx)
    var CONFIG = {"hostname":"example.com","root":"/","version":"1.9.1","typing":{"enable":true,"typeSpeed":70,"cursorChar":"-","loop":false,"scope":[]},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"left","visible":"hover","icon":""},"progressbar":{"enable":true,"height_px":3,"color":"#29d","options":{"showSpinner":false,"trickleSpeed":100}},"code_language":{"enable":true,"default":"TEXT"},"copy_btn":true,"image_caption":{"enable":true},"image_zoom":{"enable":true,"img_url_replace":["",""]},"toc":{"enable":true,"placement":"right","headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":0},"lazyload":{"enable":true,"loading_img":"/img/loading.gif","onlypost":false,"offset_factor":2},"web_analytics":{"enable":true,"follow_dnt":true,"baidu":null,"google":null,"gtag":null,"tencent":{"sid":null,"cid":null},"woyaola":null,"cnzz":null,"leancloud":{"app_id":null,"app_key":null,"server_url":null,"path":"window.location.pathname","ignore_local":false}},"search_path":"/local-search.xml"};

    if (CONFIG.web_analytics.follow_dnt) {
      var dntVal = navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack;
      Fluid.ctx.dnt = dntVal && (dntVal.startsWith('1') || dntVal.startsWith('yes') || dntVal.startsWith('on'));
    }
  </script>
  <script  src="/js/utils.js" ></script>
  <script  src="/js/color-schema.js" ></script>

  
<meta name="generator" content="Hexo 6.3.0"></head>


<body>
  

  <header>
    

<div class="header-inner" style="height: 70vh;">
  <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand" href="/">
      <strong>My Blog</strong>
    </a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/">
                <i class="iconfont icon-home-fill"></i>
                首页
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/archives/">
                <i class="iconfont icon-archive-fill"></i>
                归档
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/categories/">
                <i class="iconfont icon-category-fill"></i>
                分类
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/tags/">
                <i class="iconfont icon-tags-fill"></i>
                标签
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/about/">
                <i class="iconfont icon-user-fill"></i>
                关于
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search">
              &nbsp;<i class="iconfont icon-search"></i>&nbsp;
            </a>
          </li>
          
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle">&nbsp;<i
                class="iconfont icon-dark" id="color-toggle-icon"></i>&nbsp;</a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

  

<div id="banner" class="banner" parallax=true
     style="background: url('/img/default.jpg') no-repeat center center; background-size: cover;">
  <div class="full-bg-img">
    <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.3)">
      <div class="banner-text text-center fade-in-up">
        <div class="h2">
          
            <span id="subtitle" data-typed-text="深度学习基础(上)"></span>
          
        </div>

        
          
  <div class="mt-3">
    
    
      <span class="post-meta">
        <i class="iconfont icon-date-fill" aria-hidden="true"></i>
        <time datetime="2023-07-13 09:30" pubdate>
          2023年7月13日 上午
        </time>
      </span>
    
  </div>

  <div class="mt-1">
    
      <span class="post-meta mr-2">
        <i class="iconfont icon-chart"></i>
        
          6.5k 字
        
      </span>
    

    
      <span class="post-meta mr-2">
        <i class="iconfont icon-clock-fill"></i>
        
        
        
          55 分钟
        
      </span>
    

    
    
      
        <span id="busuanzi_container_page_pv" style="display: none">
          <i class="iconfont icon-eye" aria-hidden="true"></i>
          <span id="busuanzi_value_page_pv"></span> 次
        </span>
        
      
    
  </div>


        
      </div>

      
    </div>
  </div>
</div>

</div>

  </header>

  <main>
    
      

<div class="container-fluid nopadding-x">
  <div class="row nomargin-x">
    <div class="side-col d-none d-lg-block col-lg-2">
      

    </div>

    <div class="col-lg-8 nopadding-x-md">
      <div class="container nopadding-x-md" id="board-ctn">
        <div id="board">
          <article class="post-content mx-auto">
            <!-- SEO header -->
            <h1 style="display: none">深度学习基础(上)</h1>
            
              <p class="note note-info">
                
                  
                    本文最后更新于：2023年7月14日 上午
                  
                
              </p>
            
            <div class="markdown-body">
              
              <h1 id="前言部分"><a href="#前言部分" class="headerlink" title="前言部分"></a>前言部分</h1><p>哎哟自从开始实习之后，就觉得深度学习真的越来越不香了，申NUS又需要科研bg，没办法重操就业。打算暑假发完就再也不碰了。有一种被迫跟前妻生孩子的感觉，，，好荒谬。</p>
<p>具体安排就是暑假先弄BYOL啥的，学一下Hadoop和GaussDB吧，科研bg暑假结束算是走完了。然后下学期开始考GRE啥的吧，提提GPA。然后寒假开始还是弄弄实习经历吧。<br>感觉下周开始就得做实验了。</p>
<p>啊！我88均分的烂GPA可怎么有学上啊！</p>
<p>那就开整吧。</p>
<p>本篇文章将大体简略概述深度学习基础。主要目的是复习。因此深度不深，广度比较高。因此本篇适合有深度学习基础的经验者。</p>
<hr>
<h1 id="一、什么是深度学习？"><a href="#一、什么是深度学习？" class="headerlink" title="一、什么是深度学习？"></a>一、什么是深度学习？</h1><ul>
<li><p><strong>定义</strong>：一般是指训练多层网络结构对未知数据进行分类或者回归</p>
</li>
<li><p><strong>分类</strong>：</p>
<ol>
<li><strong>有监督学习方法</strong>：有标签的那种，前馈网络、卷积神经网络、循环神经网络。</li>
<li><strong>无监督学习方法</strong>：自编码器啥的。</li>
</ol>
</li>
<li><p><strong>思想</strong>：<br>  通过构建多层网络，对目标进行多层表示，以期通过多层的高层次特征来表示数据的抽象语义信息，获得更好特征鲁棒性。</p>
<blockquote>
<p><strong>鲁棒性：</strong> 鲁棒是Robust的音译，也就是健壮和强壮的意思。它也是在异常和危险情况下系统生存的能力。<br>            AI模型的鲁棒可以理解为模型对数据变化的容忍度。假设数据出现较小偏差，只对模型输出产生较小的影响，则称模型是鲁棒的。  </p>
</blockquote>
</li>
<li><p><strong>主要应用</strong>：都在应用。图像处理（cv）：分类检测回归识别。语音识别、nlp什么的。</p>
</li>
<li><p><strong>主要术语</strong>: </p>
<ol>
<li><strong>泛化能力</strong>：是指模型依据训练时采用的模型，针对未见过的新数据做出争取预测的能力。</li>
<li><strong>过拟合</strong>：创建的模型与训练数据过于匹配，以至于模型无法根据新数据做出正确的预测。</li>
<li><strong>batch</strong>：模型训练一次迭代使用的样本集</li>
<li><strong>参数</strong>：机器学习自己训练得到的模型变量，例如：权重；</li>
<li><strong>超参数</strong>：在模型训练的连续过程中，需要人工指正的。eg：学习率</li>
<li><strong>特征工程</strong>：指确定哪些特征可能在训练模型方面非常有用，然后将日志文件及其他来源的原始数据转换为所需的特征。特征工程有时称为特征提取。</li>
<li><strong>l1正则化</strong>：一种正则化，根据权重的绝对值的总和，来惩罚权重。在以来稀疏特征的模型中，L1正则化有助于使不相关或几乎不相关的特征的权重正好为0，从而将这些特征从模型中移除。与L2正则化相对。</li>
<li><strong>l2正则化</strong>：一种正则化，根据权重的平方和，来惩罚权重。L2正则化有助于使离群值（具有较大正值或较小负责）权重接近于0，但又不正好为0。在线性模型中，L2正则化始终可以进行泛化。</li>
</ol>
</li>
</ul>
<hr>
<h1 id="二、神经网络基础"><a href="#二、神经网络基础" class="headerlink" title="二、神经网络基础"></a>二、神经网络基础</h1><h2 id="2-1-神经网络组成"><a href="#2-1-神经网络组成" class="headerlink" title="2.1 神经网络组成"></a>2.1 神经网络组成</h2><p><strong>感知机</strong>——&gt;<strong>神经元</strong><br><img src="/2023/07/13/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/img.png" srcset="/img/loading.gif" lazyload alt="感知机模型.png"><br>这个是一个很简单的感知机模型。<br>可以理解成：我们人的神经元首先接收到一些信号，这些信号通过这些树突(dendrite)组织，<br>树突组织接收到这些信号送到细胞里边的细胞核(nucleus)，这些细胞核对接收到的这些信号， 比如说眼睛接收到的光学啊，或者耳朵接收到的声音信号，<br>到树突的时候会产生一些微弱的生物电，那么就形成这样的一些刺激，<br>那么在细胞核里边对这些收集到的接收到的刺激进行综合的处理，当他的信号达到了一定的阈值之后，那么他就会被激活，就会产生一个刺激的输出。</p>
<p>一个简单的感知机单元的与非门表示如下：<br><img src="/2023/07/13/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/img_1.png" srcset="/img/loading.gif" lazyload alt="感知机与非门.png"><br>当输入为0，1时，感知机输出为0<em>（-2）+1</em>（-2）+3&#x3D;1</p>
<p>复杂一些的感知机由简单的感知机单元组合而成：<br><img src="/2023/07/13/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/img_2.png" srcset="/img/loading.gif" lazyload alt="复杂点的感知机模型.png"></p>
<hr>
<p><strong>多层感知机</strong>  </p>
<p>多层感知机由感知机推广而来，最主要的特点是有多个神经元层，因此也叫深度神经网络。相比于单独的感知机，多层感知机的第i层的每个神经元和第i-1层的每个神经元都有连接。<br><img src="/2023/07/13/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/img_3.png" srcset="/img/loading.gif" lazyload alt="多层感知机.png"></p>
<p>当然输出层可以不止有1个神经元：</p>
<p><img src="/2023/07/13/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/img_4.png" srcset="/img/loading.gif" lazyload alt="多输出层.png"></p>
<hr>
<h2 id="2-2-关于前向传播"><a href="#2-2-关于前向传播" class="headerlink" title="2.2 关于前向传播"></a>2.2 关于前向传播</h2><p>神经网络的计算主要有两种：  </p>
<ol>
<li><strong>前向传播（FP）</strong>：作用于每一层的输入,通过逐层计算得到输出结果</li>
<li><strong>反向传播（BP）</strong>：作用于网络的输出，通过计算梯度由深到浅更新网络参数</li>
</ol>
<p><strong>前向传播</strong>：<br><img src="/2023/07/13/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/img_5.png" srcset="/img/loading.gif" lazyload alt="前向传播.png"></p>
<p><strong>反向传播</strong>：<br>这个有点复杂，具体来说流程就是，正向传播求损失，反向传播回传误差，然后求梯度。更新参数。<br>简单举个例子吧：<br><img src="/2023/07/13/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/img_6.png" srcset="/img/loading.gif" lazyload alt="反向传播.png"><br>以这张图为例：<br>上图的前向传播（网络输出计算）过程如下：<br><img src="/2023/07/13/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/img_7.png" srcset="/img/loading.gif" lazyload alt="计算流程.png"></p>
<p>误差的计算方法是mse：<br><img src="/2023/07/13/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/img_8.png" srcset="/img/loading.gif" lazyload alt="mse.png"></p>
<p>具体来说们就是步步拆开的过程，先是总体求E，只需要注意每一个神经元中还有激活函数，netj和yj不是一个东西。</p>
<p>具体的计算过程是<strong>梯度下降</strong>算法：<br><img src="/2023/07/13/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/img_9.png" srcset="/img/loading.gif" lazyload alt="梯度下降算法.png"><br>举一个具体例子进行阐述吧：<br><img src="/2023/07/13/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/img_10.png" srcset="/img/loading.gif" lazyload><br>前向运算的过程如下：（使用的是sigmoid激活函数）<br><img src="/2023/07/13/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/img_11.png" srcset="/img/loading.gif" lazyload><br>下面是反向传播过程，首先要明确这是一个<strong>链式求导</strong>的过程：<br><img src="/2023/07/13/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/img_12.png" srcset="/img/loading.gif" lazyload alt="链式求导.png"><br>然后是参数更新过程：<br><img src="/2023/07/13/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/img_13.png" srcset="/img/loading.gif" lazyload alt="参数更新.png"></p>
<hr>
<h1 id="三、超参数"><a href="#三、超参数" class="headerlink" title="三、超参数"></a>三、超参数</h1><h2 id="3-1-什么是超参数？"><a href="#3-1-什么是超参数？" class="headerlink" title="3.1 什么是超参数？"></a>3.1 什么是超参数？</h2><ul>
<li><strong>超参数</strong>：在机器学习的上下文中，超参数是在开始学习过程之前设置值的参数，而不是通过训练得到的参数数据。<br>通常情况下，需要对超参数进行优化，给学习机选择一组最优超参数，以提高学习的性能和效果。</li>
</ul>
<p>这里的重点主要在于：<strong>什么是学习率？</strong><br>以上文所提到的梯度下降来说，求梯度前的n就是学习率a。<br>可以很清晰地看到，学习率决定了梯度下降的快慢，以吴恩达机器学习课程为例：<br><img src="/2023/07/13/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/img_14.png" srcset="/img/loading.gif" lazyload alt="吴恩达机器学习.png"><br>我们先看一下上面出现的唯一一条公式，公式中的θ就是代表着权重参数，新的θ会由之前的θ计算得来，这个计算过程就是为了寻找目标函数收敛到最小值。那么公式中出现的α就是当下的学习率。</p>
<p>上面两张明显的对比图就给出了学习率过大和过小的情况。</p>
<ul>
<li><p>学习率设置<strong>过小</strong>的时候，每步太小，下降速度太慢，可能要花很长的时间才会找到最小值。</p>
</li>
<li><p>学习率设置<strong>过大</strong>的时候，每步太大，虽然收敛得速度很快，可能会像图中一样，跨过或忽略了最小值，导致一直来回震荡而无法收敛。</p>
</li>
</ul>
<hr>
<h1 id="四、激活函数"><a href="#四、激活函数" class="headerlink" title="四、激活函数"></a>四、激活函数</h1><h2 id="4-1-什么是激活函数？"><a href="#4-1-什么是激活函数？" class="headerlink" title="4.1 什么是激活函数？"></a>4.1 什么是激活函数？</h2><p><strong>这个可以好好说一下</strong><br>激活函数(Activation functions)对于人工神经网络模型去学习、理解非常复杂和非线性的函数来说具有十分重要的作用。</p>
<p>它们将<strong>非线性特性</strong>引入到我们的网络中。</p>
<p>如下图，在神经元中，输入的 inputs 通过加权，求和后，还被作用了一个函数，这个函数就是激活函数。</p>
<p>引入激活函数是为了增加神经网络模型的非线性。没有激活函数的每层都相当于矩阵相乘。就算你叠加了若干层之后，无非还是个矩阵相乘罢了。</p>
<p><img src="/2023/07/13/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/img_15.png" srcset="/img/loading.gif" lazyload alt="为什么需要有激活函数.png"></p>
<hr>
<h2 id="4-2为什么要使用激活函数？"><a href="#4-2为什么要使用激活函数？" class="headerlink" title="4.2为什么要使用激活函数？"></a>4.2为什么要使用激活函数？</h2><ol>
<li>激活函数对模型学习、理解非常复杂和非线性的函数具有重要作用。</li>
<li><strong>激活函数可以引入非线性因素</strong>。如果不使用激活函数，则输出信号仅是一个简单的线性函数。<blockquote>
<p>线性函数一个一级多项式，线性方程的复杂度有限，从数据中学习复杂函数映射的能力很小。没有激活函数，神经网络将无法学习和模拟其他复杂类型的数据，例如图像、视频、音频、语音等。</p>
</blockquote>
</li>
<li>激活函数可以把当前特征空间通过一定的线性映射转换到另一个空间，让数据能够更好的被分类。</li>
</ol>
<h2 id="4-3-为什么激活函数需要非线性函数？"><a href="#4-3-为什么激活函数需要非线性函数？" class="headerlink" title="4.3 为什么激活函数需要非线性函数？"></a>4.3 为什么激活函数需要非线性函数？</h2><ul>
<li>假若网络中全部是线性部件，那么线性的组合还是线性，与单独一个线性分类器无异。这样就做不到用非线性来逼近任意函数。    </li>
<li>使用非线性激活函数，以便使网络更加强大，增加它的能力，使它可以学习<strong>复杂的事物</strong>，复杂的表单数据，以及表示输入输出之间非线性的复杂的任意函数映射。  </li>
<li>使用非线性激活函数，能够从输入输出之间生成<strong>非线性映射</strong>。</li>
</ul>
<h2 id="4-4-常见激活函数"><a href="#4-4-常见激活函数" class="headerlink" title="4.4 常见激活函数"></a>4.4 常见激活函数</h2><p><img src="/2023/07/13/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/img_16.png" srcset="/img/loading.gif" lazyload alt="常见激活函数.png"></p>
<p><img src="/2023/07/13/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/img_17.png" srcset="/img/loading.gif" lazyload alt="常见激活函数.png"></p>
<p>等等。挺多的。  </p>
<p>关于激活函数的选择，有一个经验法则，很玄学：</p>
<blockquote>
<p>如果输出是0、1值（二分类问题），则输出层选择 sigmoid 函数，然后其它的所有单元都选择 Relu 函数。这是很多激活函数的默认选择，如果在隐藏层上不确定使用哪个激活函数，那么通常会使用 Relu 激活函数。有时，也会使用 tanh 激活函数。</p>
</blockquote>
<blockquote>
<p>对于多分类问题，一般输出层选择softmax激活函数</p>
</blockquote>
<blockquote>
<p>Tanh：它非常优秀，几乎适合所有场合</p>
</blockquote>
<blockquote>
<p>ReLU：最常用的激活函数，如果不确定用哪个激活函数，就先使用ReLU或者Leaky ReLU，再尝试其他激活函数</p>
</blockquote>
<hr>
<h2 id="4-5-一些常见问题"><a href="#4-5-一些常见问题" class="headerlink" title="4.5 一些常见问题"></a>4.5 一些常见问题</h2><ul>
<li><em><strong>为什么总是能看到Relu激活函数？</strong></em></li>
</ul>
<p>答：relu函数的优点如下：</p>
<ul>
<li><strong>训练更快</strong>：使用ReLU或者Leaky ReLU的神经网络通常会比使用sigmoid或tanh的神经网络学习得更快</li>
<li><strong>避免了梯度弥散</strong>：当输入无穷大或无穷小时，sigmoid或tanh的梯度接近0，这会造成梯度弥散现象。<strong>而ReLU或者Leaky ReLU在输入大于0的部分，导数均为一个常数，避免了梯度弥散。</strong></li>
<li><strong>单侧抑制，稀疏性</strong>：当输入小于0时，ReLU的梯度为0，神经元不会训练。模拟神经元的激活率是很低的这一特性，对输入信号的少部分进行选择性相应，正是这样的稀疏性提高了网络的性能。</li>
</ul>
<p>relu函数的缺点如下：</p>
<ul>
<li>在输入小于0的时候，即使有很大的梯度传播过来也会戛然而止。</li>
</ul>
<hr>
<ul>
<li><em><strong>为什么为什么tanh收敛速度比sigmoid快？</strong></em></li>
</ul>
<p>这与激活函数的求导有关：</p>
<p>tanh′(x)&#x3D;1−tanh(x)^2 ，导数范围在（0，1）</p>
<p>sigmoid’(x)&#x3D;s(x)(1-s(x))sigmoid<br>′<br> (x)&#x3D;s(x)(1−s(x))，导数范围在（0，1&#x2F;4）</p>
<p>由此可见，tanh(x)梯度消失的问题比sigmoid(x)轻，所以tanh收敛速度比较快。</p>
<hr>
<h1 id="五、损失函数"><a href="#五、损失函数" class="headerlink" title="五、损失函数"></a>五、损失函数</h1><h2 id="5-1-损失函数定义"><a href="#5-1-损失函数定义" class="headerlink" title="5.1 损失函数定义"></a>5.1 损失函数定义</h2><p>在机器学习任务中，大部分监督学习算法都会有一个目标函数 (Objective Function),算法对该目标函数进行优化，称为优化算法的过程。 例如在分类或者回归任务中，使用损失函数( Loss Function )作为其目标函数对算法模型进行优化 。  </p>
<p>在BP神经网络中，一般推导中，使用均方误差作为损失函数，而在实际中，常用交叉熵作为损失函数。如下图所示，我们可以清晰地观察到不同的损失函数在梯度下降过程中的收敛速度和性能都是不同的。</p>
<ol>
<li>均方误差作为损失函数收敛速度慢，可能会陷入局部最优解；</li>
<li>而交叉熵作为损失函数的收敛速度比均方误差快，且较为容易找到函数最优解.</li>
</ol>
<p>因此了解损失函数的类型并掌握损失函数的使用技巧，有助于加深对深度学习的认知.</p>
<p><img src="/2023/07/13/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/img0.png" srcset="/img/loading.gif" lazyload></p>
<p> 对于分类和回归模型使用的损失函数不同，下面将分别进行介绍。</p>
<hr>
<h2 id="5-2-回归损失函数"><a href="#5-2-回归损失函数" class="headerlink" title="5.2 回归损失函数"></a>5.2 回归损失函数</h2><p>(1)<strong>均方误差损失函数</strong></p>
<p>均方误差(Mean Squared Error Loss, MSE)损失函数定义如下：<br><img src="/2023/07/13/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/img1.png" srcset="/img/loading.gif" lazyload alt="mse.png"></p>
<p>这里说一下MSE是无法处理梯度消失问题的。先说说什么是梯度消失。</p>
<blockquote>
<p>梯度消失是在神经网络训练过程中经常遇到的问题之一。<br>当使用基于梯度的优化算法（如反向传播）来更新神经网络的权重时，梯度消失指的是在<strong>网络的较早层（通常是深层）中，梯度逐渐变得非常小，接近于零，甚至完全消失。</strong><br>梯度消失的主要原因是使用某些激活函数（如Sigmoid或Tanh）时，这些函数的导数在输入值非常大或非常小的情况下会趋近于零。当反向传播的梯度通过多个层传递时，梯度值会不断缩小，最终导致较早层的梯度非常小或消失。</p>
</blockquote>
<p>而MSE（均方误差）是一种常用的损失函数，用于衡量预测值与目标值之间的差异。MSE本身并不能直接解决梯度消失问题。</p>
<p>代码示例</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">mean_squared_error</span>(<span class="hljs-params">y_true, y_pred</span>):<br>    <span class="hljs-keyword">return</span> np.mean(np.square(y_pred - y_true), axis=-<span class="hljs-number">1</span>)<br></code></pre></td></tr></table></figure>
<p>(2) <strong>平均绝对误差损失函数</strong></p>
<p>平均绝对误差(Mean Absolute Error Loss, MAE)损失函数定义如下：</p>
<p><img src="/2023/07/13/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/img2.png" srcset="/img/loading.gif" lazyload alt="mae.png"></p>
<p>代码示例</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">mean_absolute_error</span>(<span class="hljs-params">y_true, y_pred</span>):<br>    <span class="hljs-keyword">return</span> np.mean(np.<span class="hljs-built_in">abs</span>(y_pred - y_true), axis=-<span class="hljs-number">1</span>)<br></code></pre></td></tr></table></figure>
<p>(3) <strong>均方误差对数损失函数</strong></p>
<p>均方误差对数(Mean Squared Log Error Loss, MSLE)损失函数定义如下：<br><img src="/2023/07/13/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/img3.png" srcset="/img/loading.gif" lazyload alt="msle.png"></p>
<p>代码示例</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">mean_squared_logarithmic_error</span>(<span class="hljs-params">y_true, y_pred</span>):<br>    first_log = np.log(np.clip(y_pred, <span class="hljs-number">10e-6</span>, <span class="hljs-literal">None</span>) + <span class="hljs-number">1.</span>)<br>    second_log = np.log(np.clip(y_true, <span class="hljs-number">10e-6</span>, <span class="hljs-literal">None</span>) + <span class="hljs-number">1.</span>)<br>    <span class="hljs-keyword">return</span> np.mean(np.square(first_log - second_log), axis=-<span class="hljs-number">1</span>)<br></code></pre></td></tr></table></figure>
<p>(4) 平均绝对百分比误差损失函数</p>
<p>平均绝对百分比(Mean Absolute Percentage Error Loss, MAPE)误差损失函数定义如下：<br><img src="/2023/07/13/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/img4.png" srcset="/img/loading.gif" lazyload alt="mape.png"></p>
<p>代码示例</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">mean_absolute_percentage_error</span>(<span class="hljs-params">y_true, y_pred</span>):<br>    diff = np.<span class="hljs-built_in">abs</span>((y_pred - y_true) / np.clip(np.<span class="hljs-built_in">abs</span>(y_true), <span class="hljs-number">10e-6</span>, <span class="hljs-literal">None</span>))<br>    <span class="hljs-keyword">return</span> <span class="hljs-number">100</span> * np.mean(diff, axis=-<span class="hljs-number">1</span>)<br></code></pre></td></tr></table></figure>
<p>(5) 小结<br>    <strong>均方误差损失函数是使用最广泛的</strong>，并且在大部分情况下，均方误差有着不错的性能，因此被用作损失函数的<strong>基本衡量指标。</strong><br>MAE 则会比较有效地惩罚异常值，如果数据异常值较多，需要考虑使用平均绝对误差损失作为损失函数。<br>    一般情况下，为了不让数据出现太多异常值，可以对数据进行预处理操作。 均方误差对数损失与均方误差的计算过程类似，多了对每个输出数据进行对数计算，<strong>目的是缩小函数输出的范围值</strong>。 </p>
<p>平均绝对百分比误差损失则计算<strong>预测值与真实值的相对误差</strong>。均方误差对数损失与平均绝对百分比误差损失实际上是用来处理<strong>大范围数据</strong>的， 但是在神经网络中，我们常把输入数据归一化到一个合理范围([−1,1])，然后再使用均方误差或者平均绝对误差损失来计算损失。</p>
<hr>
<h2 id="5-3-分类损失函数"><a href="#5-3-分类损失函数" class="headerlink" title="5.3 分类损失函数"></a>5.3 分类损失函数</h2><p><strong>这一部分，了解就行了，毕竟我也不会</strong></p>
<p>(1) Logistic损失函数</p>
<p>Logistic损失函数定义如下：</p>
<p><img src="/2023/07/13/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/img5.png" srcset="/img/loading.gif" lazyload alt="Logistic.png"></p>
<p>(2) 负对数似然损失函数</p>
<p>负对数似然损失函数(Negative Log Likelihood Loss)定义如下：</p>
<p><img src="/2023/07/13/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/img6.png" srcset="/img/loading.gif" lazyload alt="负对数似然.png"></p>
<p> (3) 交叉熵损失函数</p>
<p><strong>这个比较重要 记一下</strong></p>
<p>Logistic损失函数和负对数似然损失函数只能处理二分类问题，对于两个分类扩展到M个分类，使用交叉熵损失函数(Cross Entropy Loss)，其定义如下：</p>
<p><img src="/2023/07/13/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/img7.png" srcset="/img/loading.gif" lazyload alt="交叉熵.png"></p>
<p> 代码示例</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">cross_entropy</span>(<span class="hljs-params">y_true, y_pred</span>):<br>    <span class="hljs-keyword">return</span> -np.mean(y_true * np.log(y_pred + <span class="hljs-number">10e-6</span>))<br></code></pre></td></tr></table></figure>

<hr>
<h2 id="5-4-神经网络中常用的损失函数"><a href="#5-4-神经网络中常用的损失函数" class="headerlink" title="5.4 神经网络中常用的损失函数"></a>5.4 神经网络中常用的损失函数</h2><p>神经网络中的损失函数可以自定义，前提是<strong>需要考虑数据本身和用于求解的优化方案</strong>。</p>
<p>换句话说，自定义损失函数<strong>需要考虑输入的数据形式和对损失函数求导的算法</strong>。自定义损失函数其实是有些难度的，在实际工程项目上，结合激活函数来选择损失函数是常见的做法，常用组合有以下 3 种 。</p>
<ul>
<li><strong>ReLU + MSE</strong></li>
</ul>
<p>均方误差损失函数<strong>无法处理梯度消失问题</strong>，而使用 Leak ReLU 激活函数能够减少计算时梯度消失的问题.</p>
<blockquote>
<p>因此在神经网络中如果需要使用均方误差损失函数，一般采用 Leak ReLU 等可以减少梯度消失的激活函数。另外，由于均方误差具有普遍性，一般作为衡量损失值的标准，因此使用均方误差作为损失函数表现既不会太好也不至于太差。</p>
</blockquote>
<ul>
<li><strong>Sigmoid + Logistic</strong></li>
</ul>
<p>Sigmoid 函数会引起<strong>梯度消失问题</strong></p>
<blockquote>
<p>根据链式求导法，Sigmoid 函数求导后由多个［0, 1］范围的数进行连乘，如其导数形式为 ，当其中一个数很小时，连成后会无限趋近于零直至最后消失。而类 Logistic 损失函数求导时，加上对数后连乘操作转化为求和操作，在一定程度上避免了梯度消失，所以我们经常可以看到 Sigmoid 激活函数＋交叉摘损失函数 的组合。</p>
</blockquote>
<ul>
<li><strong>Softmax + Logisitc</strong></li>
</ul>
<p>在数学上，Softmax 激活函数会<strong>返回输出类的互斥概率分布</strong>，也就是能把离散的输出转换为一个同分布互斥的概率，如(0.2, 0.8)。</p>
<blockquote>
<p>另外，Logisitc 损失函数是基于概率的最大似然估计函数而来的，因此输出概率化能够更加方便优化算法进行求导和计算，所以我们经常可以看到输出层用 Softmax 激活函数＋交叉熵损失函数 的组合。</p>
</blockquote>
<h2 id="5-5-激活函数、损失函数、优化函数的区别"><a href="#5-5-激活函数、损失函数、优化函数的区别" class="headerlink" title="5.5 激活函数、损失函数、优化函数的区别"></a>5.5 激活函数、损失函数、优化函数的区别</h2><ol>
<li><p><strong>激活函数</strong>：将神经网络上一层的输入，经过神经网络层的非线性变换转换后，通过激活函数，得到输出。常见的激活函数包括：sigmoid, tanh, relu等。</p>
</li>
<li><p><strong>损失函数</strong>：度量神经网络的输出的预测值，与实际值之间的差距的一种方式。常见的损失函数包括：最小二乘损失函数、交叉熵损失函数、回归中使用的smooth L1损失函数等。</p>
</li>
<li><p><strong>优化函数</strong>：也就是如何把损失值从神经网络的最外层传递到最前面。如最基础的梯度下降算法，随机梯度下降算法，批量梯度下降算法，带动量的梯度下降算法，Adagrad，Adadelta，Adam等。</p>
</li>
</ol>

              
            </div>
            <hr/>
            <div>
              <div class="post-metas my-3">
  
    <div class="post-meta mr-3 d-flex align-items-center">
      <i class="iconfont icon-category"></i>
      

<span class="category-chains">
  
  
    
      <span class="category-chain">
        
  <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" class="category-chain-item">深度学习</a>
  
  

      </span>
    
  
</span>

    </div>
  
  
    <div class="post-meta">
      <i class="iconfont icon-tags"></i>
      
        <a href="/tags/CV/">#CV</a>
      
        <a href="/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/">#神经网络</a>
      
        <a href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">#深度学习</a>
      
    </div>
  
</div>


              
  

  <div class="license-box my-3">
    <div class="license-title">
      <div>深度学习基础(上)</div>
      <div>http://example.com/2023/07/13/深度学习基础/</div>
    </div>
    <div class="license-meta">
      
        <div class="license-meta-item">
          <div>作者</div>
          <div>yuanxi</div>
        </div>
      
      
        <div class="license-meta-item license-meta-date">
          <div>发布于</div>
          <div>2023年7月13日</div>
        </div>
      
      
        <div class="license-meta-item license-meta-date">
          <div>更新于</div>
          <div>2023年7月14日</div>
        </div>
      
      <div class="license-meta-item">
        <div>许可协议</div>
        <div>
          
            
            
              <a target="_blank" href="https://creativecommons.org/licenses/by/4.0/">
              <span class="hint--top hint--rounded" aria-label="BY - 署名">
                <i class="iconfont icon-by"></i>
              </span>
              </a>
            
          
        </div>
      </div>
    </div>
    <div class="license-icon iconfont"></div>
  </div>



              
                <div class="post-prevnext my-3">
                  <article class="post-prev col-6">
                    
                    
                      <a href="/2023/07/14/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%EF%BC%88%E4%B8%8B%EF%BC%89/" title="深度学习基础（下）">
                        <i class="iconfont icon-arrowleft"></i>
                        <span class="hidden-mobile">深度学习基础（下）</span>
                        <span class="visible-mobile">上一篇</span>
                      </a>
                    
                  </article>
                  <article class="post-next col-6">
                    
                    
                      <a href="/2023/07/12/Hadoop-%E2%80%94%E2%80%94MapReduce-%E8%AF%A6%E8%A7%A3/" title="Hadoop ——MapReduce 详解">
                        <span class="hidden-mobile">Hadoop ——MapReduce 详解</span>
                        <span class="visible-mobile">下一篇</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </article>
                </div>
              
            </div>

            
  <article id="comments" lazyload>
    
  <script type="text/javascript">
    Fluid.utils.loadComments('#comments', function() {
      var light = 'github-light';
      var dark = 'github-dark';
      var schema = document.documentElement.getAttribute('data-user-color-scheme');
      if (schema === 'dark') {
        schema = dark;
      } else {
        schema = light;
      }
      window.UtterancesThemeLight = light;
      window.UtterancesThemeDark = dark;
      var s = document.createElement('script');
      s.setAttribute('src', 'https://utteranc.es/client.js');
      s.setAttribute('repo', 'EEAlstonstar/comment-utterance');
      s.setAttribute('issue-term', 'pathname');
      
      s.setAttribute('label', 'utterances');
      
      s.setAttribute('theme', schema);
      s.setAttribute('crossorigin', 'anonymous');
      document.getElementById('comments').appendChild(s);
    })
  </script>
  <noscript>Please enable JavaScript to view the comments</noscript>


  </article>


          </article>
        </div>
      </div>
    </div>

    <div class="side-col d-none d-lg-block col-lg-2">
      
  <aside class="sidebar" style="margin-left: -1rem">
    <div id="toc">
  <p class="toc-header"><i class="iconfont icon-list"></i>&nbsp;目录</p>
  <div class="toc-body" id="toc-body"></div>
</div>



  </aside>


    </div>
  </div>
</div>





  



  



  



  



  






    

    
      <a id="scroll-top-button" aria-label="TOP" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">搜索</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v" for="local-search-input">关键词</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>

    

    
  </main>

  <footer>
    <div class="footer-inner">
  
    <div class="footer-content">
       <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a> <i class="iconfont icon-love"></i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Fluid</span></a> 
    </div>
  
  
    <div class="statistics">
  
  

  
    
      <span id="busuanzi_container_site_pv" style="display: none">
        总访问量 
        <span id="busuanzi_value_site_pv"></span>
         次
      </span>
    
    
      <span id="busuanzi_container_site_uv" style="display: none">
        总访客数 
        <span id="busuanzi_value_site_uv"></span>
         人
      </span>
    
    
  
</div>

  
  
  
</div>

  </footer>

  <!-- Scripts -->
  
  <script  src="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.js" ></script>
  <link  rel="stylesheet" href="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.css" />

  <script>
    NProgress.configure({"showSpinner":false,"trickleSpeed":100})
    NProgress.start()
    window.addEventListener('load', function() {
      NProgress.done();
    })
  </script>


<script  src="https://lib.baomitu.com/jquery/3.6.0/jquery.min.js" ></script>
<script  src="https://lib.baomitu.com/twitter-bootstrap/4.6.1/js/bootstrap.min.js" ></script>
<script  src="/js/events.js" ></script>
<script  src="/js/plugins.js" ></script>


  <script  src="https://lib.baomitu.com/typed.js/2.0.12/typed.min.js" ></script>
  <script>
    (function (window, document) {
      var typing = Fluid.plugins.typing;
      var subtitle = document.getElementById('subtitle');
      if (!subtitle || !typing) {
        return;
      }
      var text = subtitle.getAttribute('data-typed-text');
      
        typing(text);
      
    })(window, document);
  </script>




  

  

  

  

  

  

  




  
    <script  src="/js/img-lazyload.js" ></script>
  




  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/tocbot/4.18.2/tocbot.min.js', function() {
    var toc = jQuery('#toc');
    if (toc.length === 0 || !window.tocbot) { return; }
    var boardCtn = jQuery('#board-ctn');
    var boardTop = boardCtn.offset().top;

    window.tocbot.init({
      tocSelector     : '#toc-body',
      contentSelector : '.markdown-body',
      headingSelector : CONFIG.toc.headingSelector || 'h1,h2,h3,h4,h5,h6',
      linkClass       : 'tocbot-link',
      activeLinkClass : 'tocbot-active-link',
      listClass       : 'tocbot-list',
      isCollapsedClass: 'tocbot-is-collapsed',
      collapsibleClass: 'tocbot-is-collapsible',
      collapseDepth   : CONFIG.toc.collapseDepth || 0,
      scrollSmooth    : true,
      headingsOffset  : -boardTop
    });
    if (toc.find('.toc-list-item').length > 0) {
      toc.css('visibility', 'visible');
    }
  });
</script>


  <script>
  (function() {
    var enableLang = CONFIG.code_language.enable && CONFIG.code_language.default;
    var enableCopy = CONFIG.copy_btn;
    if (!enableLang && !enableCopy) {
      return;
    }

    function getBgClass(ele) {
      return Fluid.utils.getBackgroundLightness(ele) >= 0 ? 'code-widget-light' : 'code-widget-dark';
    }

    var copyTmpl = '';
    copyTmpl += '<div class="code-widget">';
    copyTmpl += 'LANG';
    copyTmpl += '</div>';
    jQuery('.markdown-body pre').each(function() {
      var $pre = jQuery(this);
      if ($pre.find('code.mermaid').length > 0) {
        return;
      }
      if ($pre.find('span.line').length > 0) {
        return;
      }

      var lang = '';

      if (enableLang) {
        lang = CONFIG.code_language.default;
        if ($pre[0].children.length > 0 && $pre[0].children[0].classList.length >= 2 && $pre.children().hasClass('hljs')) {
          lang = $pre[0].children[0].classList[1];
        } else if ($pre[0].getAttribute('data-language')) {
          lang = $pre[0].getAttribute('data-language');
        } else if ($pre.parent().hasClass('sourceCode') && $pre[0].children.length > 0 && $pre[0].children[0].classList.length >= 2) {
          lang = $pre[0].children[0].classList[1];
          $pre.parent().addClass('code-wrapper');
        } else if ($pre.parent().hasClass('markdown-body') && $pre[0].classList.length === 0) {
          $pre.wrap('<div class="code-wrapper"></div>');
        }
        lang = lang.toUpperCase().replace('NONE', CONFIG.code_language.default);
      }
      $pre.append(copyTmpl.replace('LANG', lang).replace('code-widget">',
        getBgClass($pre[0]) + (enableCopy ? ' code-widget copy-btn" data-clipboard-snippet><i class="iconfont icon-copy"></i>' : ' code-widget">')));

      if (enableCopy) {
        Fluid.utils.createScript('https://lib.baomitu.com/clipboard.js/2.0.10/clipboard.min.js', function() {
          var clipboard = new window.ClipboardJS('.copy-btn', {
            target: function(trigger) {
              var nodes = trigger.parentNode.childNodes;
              for (var i = 0; i < nodes.length; i++) {
                if (nodes[i].tagName === 'CODE') {
                  return nodes[i];
                }
              }
            }
          });
          clipboard.on('success', function(e) {
            e.clearSelection();
            e.trigger.innerHTML = e.trigger.innerHTML.replace('icon-copy', 'icon-success');
            setTimeout(function() {
              e.trigger.innerHTML = e.trigger.innerHTML.replace('icon-success', 'icon-copy');
            }, 2000);
          });
        });
      }
    });
  })();
</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/anchor-js/4.3.1/anchor.min.js', function() {
    window.anchors.options = {
      placement: CONFIG.anchorjs.placement,
      visible  : CONFIG.anchorjs.visible
    };
    if (CONFIG.anchorjs.icon) {
      window.anchors.options.icon = CONFIG.anchorjs.icon;
    }
    var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
    var res = [];
    for (var item of el) {
      res.push('.markdown-body > ' + item.trim());
    }
    if (CONFIG.anchorjs.placement === 'left') {
      window.anchors.options.class = 'anchorjs-link-left';
    }
    window.anchors.add(res.join(', '));
  });
</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js', function() {
    Fluid.plugins.fancyBox();
  });
</script>


  <script>Fluid.plugins.imageCaption();</script>

  <script  src="/js/local-search.js" ></script>

  <script defer src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" ></script>





<!-- 主题的启动项，将它保持在最底部 -->
<!-- the boot of the theme, keep it at the bottom -->
<script  src="/js/boot.js" ></script>


  

  <noscript>
    <div class="noscript-warning">博客在允许 JavaScript 运行的环境下浏览效果更佳</div>
  </noscript>
</body>
</html>
